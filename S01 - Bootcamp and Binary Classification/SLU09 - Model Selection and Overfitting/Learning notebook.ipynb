{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU09: Model Selection and Overfitting\n",
    "---\n",
    "<a id='top'></a>\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "\n",
    "### 1. [Generalization Error](#generror)\n",
    "### 2. [Model Selection](#modelselection)\n",
    "### 3. [Regularized Linear Regression](#regularization)\n",
    "### 4. [Conclusion (More on Overfitting)](#overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mlxtend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-169d6a22aefd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/batch4-workspace/S01 - Bootcamp and Binary Classification/SLU09 - Model Selection and Overfitting/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmlxtend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_decision_regions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mlxtend'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: The need for validation  \n",
    "\n",
    "Before we jump into definitions, let's try to get a couple of intuitions going. \n",
    "\n",
    "To do so, let's answer the question: \n",
    "\n",
    "> **\"How does the number of hours of TV per day relate to a person's age?\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = utils.generate_time_on_tv()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we'll start with a little plot: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df.plot(kind='scatter', x='age', y='minutes_per_day', ax=ax, alpha=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We can notice there is a clear pattern here. \n",
    "\n",
    "Now we will try to model it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High bias \n",
    "We will use a very \"inflexible\" model, simple linear regression (linear regression with just one explanatory variable). Simple linear regression requires that the answer be of the format\n",
    "\n",
    "$$ y = \\beta x + c $$\n",
    "\n",
    "<center>(a line ðŸ˜›)</center>\n",
    "\n",
    "So let's try that... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fit_lin_reg(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is no good! The reason is simple: our model had too many assumptions about the data ahead of time. It had too much **bias**.\n",
    "\n",
    "In machine learning terms, we say we've **underfit the data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Variance\n",
    "Now, in normal life the word \"bias\" has a negative connotation.\n",
    "\n",
    "So if bias is bad, maybe we should get rid of as much bias as we can, and just let the data speak for itself! Right? Ehm... \n",
    "\n",
    "We'll use a really high variance algo *(don't worry about which one just yet)*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.fit_high_variance_algo(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's no good either! The model just followed the data like an idiot, and doesn't seem to have any sort of assumptions about the data, or bias. We could say it has too much **variance**. ... oh wait that's what we wanted. (Do we though?) \n",
    "\n",
    "The learning here is... \n",
    "\n",
    "***Bias and Variance are a tradeoff***\n",
    "\n",
    "A bit of bias is necessary, and not enough will make your model **overfit** to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why does this matter?\n",
    "The goal of building machine learning models is to be able to make accurate predictions on previously unseen data; in other words, we want the model to generalize. \n",
    "\n",
    "By having too much bias, the model does not \"learn\" enough to be accurate on seen (training) data or on unseen (test) data.\n",
    "\n",
    "By having too much variance, the model may have high accuracy on training data, but it loses its power to generalize to unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='generror'></a>\n",
    "[Return to top](#top)\n",
    "# 1. Generalization Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtopics:\n",
    "1. **Decomposition**\n",
    "    1. Bias\n",
    "    2. Variance\n",
    "    3. Irreducible error\n",
    "2. **Bias-variance trade-off**\n",
    "3. **Sources of complexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Decomposing the generalization error\n",
    "\n",
    "Bias-variance decomposition is a way of analyzing an algorithm's expected generalization error, with respect to the sum of three terms:\n",
    "\n",
    "1. Bias\n",
    "\n",
    "2. Variance\n",
    "\n",
    "3. Irreducible error.\n",
    "\n",
    "As we will see, dealing with bias and variance is really about under- (high bias) and over-fitting (high variance).\n",
    "\n",
    "![dart_throwing_bias_var_tradeoff](media/dart_throwing_bias_var_tradeoff.png)\n",
    "\n",
    "*Fig.: Graphical illustration of bias and variance using dart-throwing, from [Scott Fortmann-Row's \"Understanding the Bias-Variance Trade-off\"](http://scott.fortmann-roe.com/docs/BiasVariance.html)*\n",
    "\n",
    "### 1.1.A Bias and underfitting\n",
    "\n",
    "Bias results from simplistic assumptions and a lack of flexibility: in short, we are missing parameters that would be in a correct model.\n",
    "\n",
    "Bias is always learning the same wrong thing, skewing predictions consistently across different training samples (i.e., far-off from the real value):\n",
    "\n",
    "$$ Bias = E\\big[\\hat{y} - y\\big] $$\n",
    "\n",
    "Fixing bias requires adding complexity to our models to allow them to adapt better to the data. \n",
    "\n",
    "### 1.1.B Variance and overfitting\n",
    "\n",
    "On the other side, extremely flexible models overreact to the specifics of the training data (including the random noise).\n",
    "\n",
    "Variance creeps in when we have more parameters than justified by the data and learn random things from different training samples:\n",
    "\n",
    "$$ Variance = E\\big[\\big(\\hat{y} - E[\\hat{y}]\\big)^2\\big] $$\n",
    "\n",
    "Fixing variance requires decreasing complexity to prevent the model from adapting too much to the training data.\n",
    " \n",
    "### 1.1.C Irreducible error\n",
    "\n",
    "Irreducible error is error that cannot be eliminated by building good models. It is essentially a measure of noise in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 The bias-variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a trade-off between the bias and variance. In relation to increasing model complexity, bias is reduced while variance increases.\n",
    "\n",
    "![bias_variance_trade_off](media/bias_variance_trade_off.png)\n",
    "\n",
    "*Fig.: The bias-variance tradeoff, bias is reduced and variance is increased in relation to model complexity*\n",
    "\n",
    "In theory, we reach the right level of complexity when the increase in bias is equivalent to the reduction in variance:\n",
    "\n",
    "$$ \\frac{dBias}{dComplexity} = - \\frac{dVariance}{dComplexity} $$\n",
    "\n",
    "In practice, *there is not an analytical way to find this location* and the more we (over)reach for signal, the greater the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Sources of model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model complexity arises from, among others:\n",
    "* adding new features\n",
    "* increasing the polynomial degree of the hypothesis\n",
    "* using highly flexible models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modelselection'></a>\n",
    "[Return to top](#top)\n",
    "# 2. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtopics:\n",
    "\n",
    "1. **Offline evaluation**\n",
    "    1. Leave-one-out or hold-out method\n",
    "        1. [`sklearn.model_selection.train_test_split`](#traintestsplit)\n",
    "        2. In-sample or training error\n",
    "        3. Out-of-sample or testing error\n",
    "        4. [Validation dataset](#valset)\n",
    "        5. Evaluating overfitting and underfitting\n",
    "    2. [K-Fold cross-validation](#kfolds)\n",
    "        1. `sklearn.model_selection.cross_val_score`\n",
    "    3. [Data leakage](#dataleak)\n",
    "2. [**Practical considerations**](#practical)\n",
    "    1. Training time\n",
    "    2. Prediction time\n",
    "    3. Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Offline evaluation\n",
    "\n",
    "One way to understand overfitting is by decomposing the generalization error of a model into bias and variance.\n",
    "\n",
    "We will be using data about craft beer to try to predict whether a particular beer is an [India Pale Ale (IPA)](https://en.wikipedia.org/wiki/India_pale_ale).\n",
    "\n",
    "The data was preprocessed in advance, as the original dataset was simplified and manipulated for teaching purposes. There are two features:\n",
    "* `IBU`, which stands for International Bitterness Units and is a measure of bitterness\n",
    "* `Color`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://continue.utah.edu/images/widget/lifelong/LLFW529beer-sensory-evaluation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/beer.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['Color', 'IBU']]\n",
    "y = data['IsIPA']  # <--- to be an IPA or not to be an IPA, that is the question "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick idea of how the target (IsIPA) varies with the features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(kind='scatter', x='Color', y='IBU', c='IsIPA', colormap='coolwarm', figsize=(10, 8), alpha=.5)\n",
    "plt.xlabel('Color')\n",
    "plt.ylabel('IBU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try to model the type of beer (isIPA) by using `Color` and `IBU`. \n",
    "\n",
    "We will use 3 classifiers: \n",
    "\n",
    "> `SuperConservative` - will have very **low variance**, and **high bias**  \n",
    "> `SuperFlexible` - Will have very **high variance**, and **low bias**   \n",
    "> `WellBalanced` - Will be juuuust right "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then calculate the accuracy of each of the classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `SuperConservative` (high bias) \n",
    "\n",
    "For our `SuperConservative` model we will use a `Logistic Regression`. \n",
    "\n",
    "Logistic regression provides an example of bias because it makes a lot of assumptions about the form of the target function.\n",
    "\n",
    "Visually, we can understand the model's inability to adjust to a non-linear decision boundary, structurally enforcing a linear one instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_super_conservative = utils.plot_super_conservative(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, preds_super_conservative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It actually doesn't do too badly, but when we look at the \"right side\" we can tell it is probably blue all the way to the top, right? \n",
    "\n",
    "Let's give it tons of flexibility! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `SuperFlexible` (high variance)  \n",
    "\n",
    "For our super flexible model, we will use a `k-Nearest Neighbors` with k=1 (don't worry about what this is yet!)\n",
    "\n",
    "The k-Nearest neighbors algorithm provides great flexibility and minimum underlying structure.\n",
    "\n",
    "The small orange *pockets* or *islands* show that our model is overadapting to the training data and, most probably, fitting to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_super_flexible = utils.plot_super_flexible(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh. Right. It definitely figured out that the top right is blue, but it also did some pretty crazy things. It's pretty clear that it's fitting noise. \n",
    "\n",
    "What about the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, preds_super_flexible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model has perfect accuracy, but when we look at the decision regions we know that this is not a \"good\" model. We will see how to reconcile these seemingly conflicting ideas in a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `WellBalanced` (sort of better)\n",
    "\n",
    "For the well balanced one, we will use a `K-Nearest Neighbors` with k=9 \n",
    "\n",
    "A key part of the k-NN algorithm is the choice of *k*: the number of nearest numbers in which to base the prediction.\n",
    "\n",
    "Increasing *k* results in considering more observations in each prediction and makes the model more rigid, for good effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_just_right = utils.plot_just_right(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, preds_just_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So... how do we choose? \n",
    "\n",
    "The irony is that when we calculate the accuracy (or any other metric) of these models, our \"best\" one is`SuperFlexible`! The reason is simple: it has clearly overfit the data, but if we evaluate (calculate metrics on) it, we're using that data again, so it will be right. \n",
    "\n",
    "Plotting helps us see that the SuperFlexible model is not actually best. Still, comparing different models by plotting decision boundaries is not very scientific, especially at higher dimensions.\n",
    "\n",
    "There must be a better way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The need for validation\n",
    "\n",
    "Given the above, we need to validate our models after training, to know if they are any good:\n",
    "1. Our assumptions may not hold (that is, we trained a garbage model) or there may be better models\n",
    "2. We may be learning parameters that don't generalize for the entire population (i.e., statistical noise).\n",
    "\n",
    "Remember, our goal is to approximate the true, universal target function *f* and we need our model to generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='traintestsplit'></a> [Top of section](#modelselection)\n",
    "## 2.1.A Train-test split (holdout method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most obvious way to solve this problem is to separate your dataset into two parts: \n",
    "- the training set, where we will find out which model to use \n",
    "- the test set, where we will make sure we didn't just overfit the training set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Machine_learning_nutshell_--_Split_into_train-test_set.svg/2000px-Machine_learning_nutshell_--_Split_into_train-test_set.svg.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are other, more sophisticated approaches to measuring generalization power of machine learning models, from a basic data science perspective, having a held-out test set that is only used at the end of the process is one of the most sacred concepts. \n",
    "\n",
    "Someone brilliant (and whose name I can't recall) once said: \n",
    "> _**\"Every time you use your test set your data dies a little\"**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is because every time you use your test set you lose the ability to tell whether you are overfitting the data you happen to have at hand. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample-error (ISE) or training error\n",
    "\n",
    "\n",
    "The in-sample-error is how well our model performs on the training data.\n",
    "\n",
    "We will measure the error rate for each model in the simplest way, by computing the fractions of misclassified cases.\n",
    "\n",
    "Remember our 3 classifiers? Let's calculate the in-sample-error for each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {'SuperConservative': LogisticRegression(),\n",
    "        'WellBalanced': KNeighborsClassifier(n_neighbors=9),\n",
    "        'SuperFlexible': KNeighborsClassifier(n_neighbors=1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    error_rate = 1 - accuracy_score(y, y_pred)\n",
    "    return round(error_rate * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make the first 800 rows (80%) training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:800]\n",
    "y_train = y[:800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make the last 200 rows (20%) test data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our model's performance on the training data is a common mistake and underestimates the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = {}\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error[key] = classification_error(clf, X_train, y_train)\n",
    "\n",
    "pd.Series(training_error).plot(figsize=(7, 5), kind='bar', rot=0)\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Training error per classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mean, clearly the `SuperFlexible` model is the best one! Right? (wrong, as we've seen before).\n",
    "\n",
    "Next, we'll measure the out of sample error of each of the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample error (OSE) or testing error\n",
    "\n",
    "The out-of-sample error measures how well the model performs on previsouly unseen data and if it's picking up patterns that generalize well.\n",
    "\n",
    "Ideally, both training and test errors are low and close to one another.\n",
    "\n",
    "* *Underfitted* models tend to perform poorly on both train and test data, having large (and similar) in-sample- and out-of-sample errors.\n",
    "\n",
    "* *Overfitting* is detected when a model that performs on training data but not quite so well in the test set: the bigger the gap, the greater the overfitting.\n",
    "\n",
    "![train_test_error](media/train_test_error.png)\n",
    "\n",
    "*Fig.: How training and test errors behave in regards to model complexity, bias and variance*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But okay, let's see how our models perform on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_error = {}\n",
    "for key, clf in clfs.items():\n",
    "    test_set_error[key] = classification_error(clf, X_test, y_test)\n",
    "\n",
    "pd.Series(test_set_error).plot(figsize=(7, 5), kind='bar', rot=0)\n",
    "plt.ylabel('Test set Error')\n",
    "plt.title('Test set error per classifier')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before, there are different techniques to measure the testing error. We will focus on:\n",
    "1. Train-test split\n",
    "2. Validation set\n",
    "3. Cross-validation\n",
    "4. Bootstrapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split (aka holdout method)\n",
    "\n",
    "The simplest solution is to leave a random subset of the data aside from the beginning to test our final model at the end.\n",
    "\n",
    "![test_set](media/test_set.png)\n",
    "\n",
    "*Fig.: Test set illustrated, you holdout a significant chunk of the data for testing your model in the end*\n",
    "\n",
    "\n",
    "![train_test_set](media/train_test_split.png)\n",
    "\n",
    "*Fig.: Workflow with test and training sets*\n",
    "\n",
    "After evaluation, you can relearn your final chosen model on the whole data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn has some amazing functionality to help you with model selection. Here we are importing a data scientist's best friend, `train_test_split`. This function randomly assigns data points to the train or test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# You can specify the percentage of the full dataset you want to reserve for testing, here we are using 40%\n",
    "# Setting the random state fixes the randomness of train/test split so the sets are reproducible \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "print(f\"---\\nNumber of observations:\\nTrain: {X_train.shape[0]} | Test: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the classification error on both the train and test sets for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(X_train, y_train, X_test, y_test, clf):\n",
    "    training_error = classification_error(clf, X_train, y_train)\n",
    "    test_error = classification_error(clf, X_test, y_test)\n",
    "    return training_error, test_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    print(f'---\\n{key} error:\\nTrain: {training_error}% | Test: {test_error}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly recap, SuperConservative has high bias and has both high train and test error. It is underfit.\n",
    "\n",
    "SuperFlexible has high variance and has low train but high test error. It is overfit.\n",
    "\n",
    "WellBalanced has a balance of bias and variance and has relatively low train and test error. It is well-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='valset'></a> [Top of section](#modelselection)\n",
    "### Validation set\n",
    "\n",
    "Given we have enough data, we can create a *validation dataset*.\n",
    "\n",
    "![validation_set](media/validation_set.png)\n",
    "\n",
    "*Fig.: Validation set as compared with the holdout approach*\n",
    "\n",
    "![validation_split](media/validation_split.png)\n",
    "\n",
    "*Fig.: Workflow with test, validation and training sets*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a validation set and a test set, we use `train_test_split` twice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First separate how much data you want to reserve for val + test, we are using 40% again\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=1234)\n",
    "# Then separate the temporary val + test set, typically they are the same size so we are using 50%\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=1234)\n",
    "del X_temp, y_temp\n",
    "\n",
    "print(f\"Number of observations:\\nTrain: {X_train.shape[0]} | Test: {X_test.shape[0]} | Validation: {X_val.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute the classification error on the train, validation, and test sets for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_metrics(X_train, y_train, X_test, y_test, X_val, y_val, clf):\n",
    "    training_error, test_error = compute_metrics(X_train, y_train, X_test, y_test, clf)\n",
    "    validation_error = classification_error(clf, X_val, y_val)\n",
    "    return training_error, test_error, validation_error\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    training_error, test_error, validation_error = compute_validation_metrics(\n",
    "                                        X_train, y_train, X_test, y_test, X_val, y_val, clf)\n",
    "    print(f'---\\n{key} error:\\nTrain: {training_error}% | Validation: {validation_error}% | Test: {test_error}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be wondering, how is this validation set different from the test set? Don't we basically just have two test sets?\n",
    "\n",
    "Typically, a validation set will be used to tune parameters of the model, and then final evaluation of OSE will be done on the test set. \n",
    "\n",
    "To demonstrate this, we will use the validation set we created above to find the optimal value of *k* for our KNN classifier. \n",
    "\n",
    "Again, don't worry about what this means exactly, just know that with increasing *k*, the model becomes less flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the errors\n",
    "error_dict = {}\n",
    "# store the classifiers so we can retrieve the best one later\n",
    "clf_dict = {}\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_error, validation_error = compute_metrics(X_train, y_train, X_val, y_val, knn)\n",
    "    error_dict[k] = {'train_error': train_error, 'validation_error': validation_error}\n",
    "    clf_dict[k] = knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(list(error_dict.keys()), [d['train_error'] for d in error_dict.values()], label='train_error')\n",
    "plt.plot(list(error_dict.keys()), [d['validation_error'] for d in error_dict.values()], label='val_error')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the graph, it looks like `k=5` is actually the best! Now we will use this value for *k* in evaluating the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_5 = clf_dict[5]\n",
    "test_error = classification_error(knn_5, X_test, y_test)\n",
    "print(f'KNN, k=5 error:\\nTrain: {error_dict[5][\"train_error\"]}% | Validation: {error_dict[5][\"validation_error\"]}% '\\\n",
    "      f'| Test: {test_error}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty cool! We were able to tune the parameter *k* on our validation set, and the OSE (test error) actually dropped from the `WellBalanced` (k=9) model we trained before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kfolds'></a> [Top of section](#modelselection)\n",
    "## 2.1.B k-Fold Cross-validation\n",
    "\n",
    "Test error results can be subject to great variability, especially for smaller datasets, depending on how we split the data (i.e., which observations go to train and which go to val/test).\n",
    "\n",
    "Also, and quite obviously, holding out *more* data reduces the amount available for training, possibly leading us to *overestimate* the test error.\n",
    "\n",
    "One way to mitigate this is to use ***k*-fold cross validation.**\n",
    "\n",
    "In *k*-fold cross validation:\n",
    "1. The original sample is randomly partitioned into *k* equal sized parts, or subsamples\n",
    "\n",
    "2. Each time, we leave out part *k*, fit the model to the other *k*-1 subsets combined in a single dataset, and then test the model against the left out *k*th part\n",
    "\n",
    "3. This is done for each *k* = 1, 2, ..., *K*, and then the results are combined, using, for example, the mean error.\n",
    "\n",
    "![cross_validation](media/cross_validation.png)\n",
    "\n",
    "*Fig.: Creating multiple (K=5) train and test set pairs using cross-validation*\n",
    "\n",
    "This way, we use every observation to both train and test out model: each fold is used once as validation, while the *k*-1 remaining folds form the training set.\n",
    "\n",
    "The mean of the error of every fold can be seen as a proxy for OSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, scikit-learn already has cross validation implemented for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for key, clf in clfs.items():\n",
    "    # We can specify the number of folds we want with cv\n",
    "    scores = cross_val_score(clf, X, y, cv=10, scoring=classification_error)\n",
    "    mean_error = round(np.mean(scores), 2)\n",
    "    var_error = round(np.var(scores), 2)\n",
    "    print(f'---\\n{key} validation error:\\nMean: {mean_error}% | Variance: {var_error}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, since each training set contains just part of the data, the estimated test error can, still, be biased upward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataleak'></a> [Top of section](#modelselection)\n",
    "## 2.1.C Data Leakage\n",
    "We must keep the test data aside for the entire modeling process (including data preprocessing, feature engineering, etc.), otherwise knowledge about the test set will *leak* into the model and ruin the experiment. Data leakage can be hard to detect, but if your results seem a little too good to be true, that's one sign. Ways to combat data leakage include:\n",
    "\n",
    "* Perform data preparation within your cross validation folds\n",
    "* Hold back a test dataset for final sanity check of your developed models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='practical'></a> [Top of section](#modelselection)\n",
    "# 2.2 Practical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to choosing models based on their performance, there are practical considerations that data scientists need to  factor into model selection. It is important to always keep your business case in mind, and consider if factors like speed and memory usage are more important than that extra 0.1% in accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time\n",
    "\n",
    "* Sometimes the \"best\" models can take a long time to train (for certain deep learning models, it can be as long as a few days or weeks)\n",
    "  * We need to consider if the business case warrants waiting this long for results\n",
    "* It is best practice to work towards a quick baseline, or **MVP** (Minimum Viable Product), with a simple model, and then iterate to improve it\n",
    "  * Oftentimes a simple model will be quick to train and still yield decent performance\n",
    "  * Furthermore, if your data is noisy, or if there is too much irreducible error, the most complex and advanced models will not be able to \"learn\" much more than a simple model anyway. So, it is better to try something quick before wasting time trying to train a complex model just to find out there is too much noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction time\n",
    "\n",
    "* The time your model takes to return predictions is also very important, especially in production environments\n",
    "* Again, you need to consider your use case and decide what prediction time is reasonable (i.e., do you need real-time predictions??)\n",
    "  * You can have a near-perfect model, but if it takes 30 seconds to return a prediction for one sample, a slightly worse model that takes 0.1 seconds for prediction might be better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory (and $$$)\n",
    "\n",
    "* Some complex models (again, deep learning models are a good example) occupy a lot of disk space and/or require a large amount of memory (RAM)\n",
    "  * These factors not only play into prediction time but can translate to actual costs for your business\n",
    "  * Training and then serving heavy models in production may require more expensive machines that can impact margins!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regularization'></a>\n",
    "[Return to top](#top)\n",
    "# 3. Regularized Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtopics\n",
    "\n",
    "1. Intuition and use-cases\n",
    "3. [Ridge, or L2](#ridge)\n",
    "    1. `sklearn.linear_model.Ridge`\n",
    "2. [Lasso, or L1](#lasso)\n",
    "    1. `sklearn.linear_model.Lasso`\n",
    "4. [Elastic Net](#elasticnet)\n",
    "    1. `sklearn.linear_model.ElasticNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Intuition and use-cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout your jouney into data science, it will be very common for you to deal with problems stemming from having a small dataset, noisy features and, also, a high sparsity level in that dataset (i.e. a lot of entries in your dataset will be \"missing\"). Many of the models we usually have can suffer greatly under these circumstances, especially if they have a lot of parameters to be estimated (i.e. many degrees of freedom). Having many parameters is analogous to a model having high complexity, or high variance. \n",
    "\n",
    "In this SLU, we've already talked a lot about how high variance can lead to overfitting, and we've seen how to estimate a model's level of overfitting by comparing metrics on the train and test sets.\n",
    "\n",
    "Now we'll discuss a way to actually combat overfitting during training time, **Regularization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "> *(...) regularization is the process of introducing additional information in order to solve an ill-posed problem or to avoid overfitting.*\n",
    "\n",
    "Regularization awards a model's *goodness of fit* while penalizing *model complexity* automatically, while it is fitting the data!\n",
    "\n",
    "In this notebook, we will explore $L_1$ and $L_2$ regularizers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we described the loss function of linear regression as\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n",
    "\n",
    "**This loss function has a (serious) problem: the optimization methods will adapt, as much as they can, the parameters to the training set.**\n",
    "\n",
    "To illustrate this, let's explore the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_dataset()\n",
    "original_data = data.copy()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data['x'], data['y'], c='orange', s=5)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this dataset is noisy but has a clear relation between the input and the target. Let's fit a simple linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly this model is underfit.\n",
    "\n",
    "In order to try to get a better result, let's add extra inputs: **powers of `data['x']`** (aka polynomial features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import expand_dataset, fit_and_plot_linear_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 3)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_linear_regression(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved our RÂ²! Let's get crazy and see what happens with many more powers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 10)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_and_plot_linear_regression(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our $R^2$ is even better still!\n",
    "\n",
    "Let's keep going with more powers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 20)\n",
    "fit_and_plot_linear_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 40)\n",
    "fit_and_plot_linear_regression(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 200)\n",
    "fit_and_plot_linear_regression(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that the model, when adding a large number of this type of features, starts to fit to the noise as well! This means that a test set will, very likely, produce a really bad $R^2$, eventhough we are increasing the $R^2$ in the training set (also, remember that issue with noise fitting and $R^2$?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ridge'></a> [Top of section](#regularization)\n",
    "## 3.2 Ridge Regularization ($L_2$ norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that we can do to address this overfitting is apply $L_2$ regularization in our linear regression model. This means changing the loss function from the standard MSE\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$$\n",
    "to\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\|\\beta\\|_2^2$$\n",
    "$$=$$\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_2 \\sum_{k=1}^K \\beta_k^2$$\n",
    "\n",
    "You'll notice that the left part of the loss function, $\\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2$, is still MSE. By keeping this part in the loss function, the model will continue to try to reduce this amount. If this part is low, that indicates a good fit. If it is high, that indicates a bad fit. \n",
    "\n",
    "However, the new part of the function, $\\lambda_2 \\sum_{k=1}^K \\beta_k^2$, is the sum of the squares of the betas, or the coefficients. The model will also try to reduce this amount, meaning the model will award smaller coefficients. If this part is low, that indicates a simple model. If it is high, that indicates a complex model.\n",
    "\n",
    "Ideally we will have low values for both parts, resulting in a well-fitted, simple model.\n",
    "\n",
    "In the $L_2$ loss function, $\\lambda_2$ is the strength of the regularization part, which is a parameter that can be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, $\\beta_0$ (i.e. the intercept) is excluded from the regularization expression (*k* starts at 1). \n",
    "\n",
    "This is due to certain theoretical aspects related to the intercept that are completely out of scope in here. If you are interested in knowing more about them, check the discussion in [stats.exchange.com](https://stats.stackexchange.com/questions/86991/reason-for-not-shrinking-the-bias-intercept-term-in-regression) or check this **bible** called [*Elements of Statistical Learning*](https://web.stanford.edu/~hastie/ElemStatLearn/) (there is also a MOOC for this). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll go through an example using a new class that implements this type of regularization: [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 40)\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# alpha here is the same as lambda in the loss function from above\n",
    "ridge = Ridge(normalize=True, alpha=0.0001, random_state=10)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "plt.scatter(X['x'], data['y'], c='orange', s=5)\n",
    "plt.plot(X['x'], ridge.predict(X))\n",
    "plt.title(f'Ridge Regression (RÂ²: {ridge.score(X, y)})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we are still allowing the model to have up to 40-degree polynomial features, the model doesn't look very overfit!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the coefficients of the non-regularized Linear Regression and the regularized version to see what's happening:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No regularization\n",
    "lr = LinearRegression(normalize=True)\n",
    "lr.fit(X, y)\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(range(len(lr.coef_)), [abs(coef) for coef in lr.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With regularization\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(range(len(ridge.coef_)), [abs(coef) for coef in ridge.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Ridge model is clearly less overfit than the normal Linear Regression model, and only has 3-4 significant parameters, while the LR has 10-15!\n",
    "\n",
    "From this, we see that another benefit of regularization can be increased **interpretability**. The regularized model automatically learns which features are important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of power features and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 400)\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "ridge = Ridge(normalize=True, alpha=0.0001, random_state=10)\n",
    "ridge.fit(X, y)\n",
    "\n",
    "plt.scatter(X['x'], data['y'], c='orange', s=5)\n",
    "plt.plot(X['x'], ridge.predict(X))\n",
    "plt.title(f'Ridge Regression (RÂ²: {ridge.score(X, y)})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Even after adding more features, out model didn't change (almost) anything! \n",
    "\n",
    "By the way, keep in mind that if we used the adjusted RÂ², by this time, we would have an awful RÂ² due to the number of useless features we have! If you don't believe us, let's check the coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ridge.coef_ == 0).sum() / ridge.coef_.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~44% of feature coefficients are 0! This means that those features are completely useless and, as desired, the adjusted RÂ² would greatly penalize us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lasso'></a> [Top of section](#regularization)\n",
    "## 3.3 Lasso Regularization ($L_1$ norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides $L_2$, we also have $L_1$ regularization.\n",
    "\n",
    "The loss function for $L_1$ is\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\|\\beta\\|_1^1$$\n",
    "$$=$$\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\sum_{k=1}^K \\left|\\beta_k\\right|$$\n",
    "\n",
    "This is usually called [Lasso regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html). The difference between lasso and ridge is that instead of squaring the coefficients in ridge, lasso uses the absolute value of the coefficients. This type of regression is way more aggressive in its approach to constraining coefficient magnitude. In many real world scenarios, it usually has just a few features with coefficients different from 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the same examples, this time using Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 40)\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# alpha here is the same as lambda in the loss function from above\n",
    "lasso = Lasso(normalize=True, alpha=0.0002, random_state=10)\n",
    "lasso.fit(X, y)\n",
    "\n",
    "plt.scatter(X['x'], data['y'], c='orange', s=5)\n",
    "plt.plot(X['x'], lasso.predict(X))\n",
    "plt.title(f'Lasso Regression (RÂ²: {lasso.score(X, y)})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the coefficients of the fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(len(lasso.coef_)), [abs(coef) for coef in lasso.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we saw with Ridge, just a few features (the lowest-order polynomials) are significant, and all other features are zero!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='elasticnet'></a> [Top of section](#regularization)\n",
    "## 3.4 Elastic Net Regularization ($L_1 + L_2$ norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, finally, we can have both $L_1$ and $L_2$ in what is called [Elastic Net regression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)\n",
    "\n",
    "The loss function for elastic net just adds the extra parts we added in ridge and lasso:\n",
    "\n",
    "$$J = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 + \\lambda_1 \\sum_{k=1}^K \\left|\\beta_k\\right| + \\lambda_2 \\sum_{k=1}^K \\beta_k^2$$\n",
    "\n",
    "We'll repeat the same example again using Elastic Net. This time `alpha` is the total weight of the penalty terms ($\\lambda_1 + \\lambda_2$), and we can also set the ratio of $\\lambda_1$ to $\\lambda_2$ with `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = expand_dataset(original_data, 40)\n",
    "\n",
    "X = data.drop('y', axis=1)\n",
    "y = data['y']\n",
    "\n",
    "# alpha here is the same as lambda in the loss function from above\n",
    "en = ElasticNet(normalize=True, alpha=0.00001, l1_ratio=0.5, random_state=10)\n",
    "en.fit(X, y)\n",
    "\n",
    "plt.scatter(X['x'], data['y'], c='orange', s=5)\n",
    "plt.plot(X['x'], en.predict(X))\n",
    "plt.title(f'Elastic Net Regression (RÂ²: {en.score(X, y)})')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,5))\n",
    "plt.plot(range(len(en.coef_)), [abs(coef) for coef in en.coef_], marker='o', markerfacecolor='r')\n",
    "plt.xlabel('Polynomial degree')\n",
    "plt.ylabel('Coef. magnitude');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see a similar behavior to the other regularizers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='overfit'></a>\n",
    "[Return to top](#top)\n",
    "\n",
    "# 4. Conclusion (More on Overfitting)\n",
    "\n",
    "How to deal with overfitting is one of the fundamentals that every data scientist must know, so we're going to talk about it some more! Here are some already-mentioned and additional techniques to prevent overfitting:\n",
    "\n",
    "#### Removing features\n",
    "We can manually remove features to reduce model complexity. In the beer example, it's possible that the color feature is not very informative and the model can classify IPAs on bitterness alone. So, we might try removing the color feature. Regularized linear regression can be considered a subset of this strategy since it automatically encourages the number of relevant features to be small.\n",
    "\n",
    "#### Adding more data\n",
    "If you can get your hands on it, adding additional data can make the signal in your data stronger, provided it is clean data and doesn't just add more noise. If we found some new beers to add to our training data it might help the model generalize better.\n",
    "\n",
    "#### Using a simpler model\n",
    "Using a model with fewer learnable parameters can reduce overfitting since very complex models can overreact to the specifics of the training data and learn random noise. To classify IPAs based on 2 features, choosing logistic regression over, say, a deep neural network makes sense because it is a fairly simple task. Of course, there are many cases where we need a complex model, image classification for example. Basically, choose a model that is fit to the task.\n",
    "\n",
    "#### Algorithm-specific techniques and regularization\n",
    "When learning new machine learning algorithms (like in the next units of the bootcamp!), always ask, \"How can we prevent overfitting with this model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for finishing this learning notebook! Hopefully you now have a better idea of \n",
    "1. How to choose the best models, optimizing for both bias and variance and keeping practical considerations in mind\n",
    "2. Different techniques to separate your data into train and validation/test sets, and the benefits and drawbacks of each\n",
    "3. What is overfitting, why it's bad, and ways to combat it, including with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
