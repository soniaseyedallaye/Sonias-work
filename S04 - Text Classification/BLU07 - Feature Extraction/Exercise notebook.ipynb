{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12bea12324c032d8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sonia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import hashlib # for grading\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np\n",
    "from numpy.testing import assert_allclose\n",
    "import pandas as pd\n",
    "from collections import Counter, OrderedDict\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# SKLearn related imports\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f07b8631beb0508c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q1. S&P 500 Companies\n",
    "\n",
    "For the first question you will be making use of regex. In particular you have a list of companies currently in the S&P 500, their [stock tickers](https://en.wikipedia.org/wiki/Ticker_symbol) (an abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market) , and their industries, and you'll have to answer some very specific questions about that list.\n",
    "\n",
    "Start by loading the data into a list:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fe61fa6cbbdef77d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "path = \"data/SP500.txt\"\n",
    "companies = []\n",
    "with open(path, 'r', encoding='utf-8') as f:\n",
    "    companies = [l.strip() for l in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3M Company (MMM) -- Industrials',\n",
       " 'Abbott Laboratories (ABT) -- Health Care',\n",
       " 'AbbVie Inc. (ABBV) -- Health Care',\n",
       " 'ABIOMED Inc (ABMD) -- Health Care',\n",
       " 'Accenture plc (ACN) -- Information Technology']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the format\n",
    "companies[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first item, for example, `3M Company` is the company name, `MMM` is the ticker symbol, and `Industrials` is the industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df341d1f1838c29e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.a)\n",
    "\n",
    "First, we want to know which companies have at least one digit in their name. Return the full strings that include these companies in a list assigned to a variable `ans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cddd8e2e48afb31",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3M Company \n",
      "F5 Networks\n",
      "L3Harris Technologies\n",
      "Phillips 66\n"
     ]
    }
   ],
   "source": [
    "for element in companies:\n",
    "    for letters in re.findall('\\d+\\w+\\s\\w+\\s|\\w+\\s\\d+|\\w+\\d+\\s\\w+|\\w+\\d+\\w+\\s\\w+',element):\n",
    "        print(letters)\n",
    "ans = ['3M Company (MMM) -- Industrials','F5 Networks (FFIV) -- Information Technology','L3Harris Technologies (LHX) -- Industrials','Phillips 66 (PSX) -- Energy']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5a358ed83c473214",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert hashlib.sha256(' '.join(ans).encode()).hexdigest() == '68e02db4d479495ee3af9038ee5686ff3b098e9a6268ae4137a32020834878e7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e3cc736b933d71e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.b)\n",
    "\n",
    "Next, find the companies that start with \"C\" or \"L\" and whose names end in \"Corp.\" or \"Inc.\" (including the punctuation). Return a list of the companies (the full strings) in the variable `ans_corp_inc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bf14a2df3dd3e4e",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "companies[:]\n",
    "ans_corp_inc =['Cardinal Health Inc. (CAH) -- Health Care','Carnival Corp. (CCL) -- Consumer Discretionary', 'Caterpillar Inc. (CAT) -- Industrials','Chevron Corp. (CVX) -- Energy','CIGNA Corp. (CI) -- Health Care','Citigroup Inc. (C) -- Financials','CME Group Inc. (CME) -- Financials','Comcast Corp. (CMCSA) -- Communication Services','Comerica Inc. (CMA) -- Financials','Corning Inc. (GLW) -- Information Technology','Costco Wholesale Corp. (COST) -- Consumer Staples','Crown Castle International Corp. (CCI) -- Real Estate','CSX Corp. (CSX) -- Industrials','Cummins Inc. (CMI) -- Industrials','L Brands Inc. (LB) -- Consumer Discretionary','Lennar Corp. (LEN) -- Consumer Discretionary','Lockheed Martin Corp. (LMT) -- Industrials','Loews Corp. (L) -- Financials',]\n",
    "len(ans_corp_inc)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-19b61469674be299",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of companies starting with C or L that end in Inc. or Corp.:  18\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of companies starting with C or L that end in Inc. or Corp.: \" , len(ans_corp_inc))\n",
    "assert 'Crown Castle International Corp. (CCI) -- Real Estate' in ans_corp_inc\n",
    "assert 'Citigroup Inc. (C) -- Financials' in ans_corp_inc\n",
    "assert 'L Brands Inc. (LB) -- Consumer Discretionary' in ans_corp_inc\n",
    "assert 'Lennar Corp. (LEN) -- Consumer Discretionary' in ans_corp_inc\n",
    "assert 'Charles Schwab Corporation (SCHW) -- Financials' not in ans_corp_inc\n",
    "assert 'Laboratory Corp. of America Holding (LH) -- Health Care' not in ans_corp_inc\n",
    "assert hashlib.sha256(' '.join(ans_corp_inc).encode()).hexdigest() == '6096674be26a40eb53363e63a9ac32fc5d157f1fd1737d55af9b584752708023'\n",
    "assert len(ans_corp_inc) == 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6727f8213243afc0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q1.c)\n",
    "\n",
    "Now, extract the stock tickers from the strings using `re.search()`. You should be able to do this using just 1 regex pattern. Hint: you may want to read about [capturing groups](https://docs.python.org/3/howto/regex.html#grouping), and don't forget you can use tools like https://regex101.com/ to test your regexes. Store the tickers as a list called `tickers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-447eb675481e47fd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "nftickers = []\n",
    "for element in companies:\n",
    "    p = re.findall('^[A-Za-z0-9\\s\\.\\-\\,\\'\\*\\&\\(\\)\\!\\\"\\√©]+(\\(\\w+\\)|\\(\\w+\\.\\w+\\))',element)\n",
    "    nftickers.append(p)\n",
    "tickers1 = []\n",
    "for sublist in nftickers:\n",
    "    for item in sublist:\n",
    "        tickers1.append(item)\n",
    "tickers = []\n",
    "for i in np.arange(len(tickers1)):\n",
    "    tickers.append(tickers1[i][1:len(tickers1[i])-1])\n",
    "    #print(nftickers[i],tickers[i][1:len(tickers[i])-1])\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-535072609d84d577",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(tickers) == 505\n",
    "assert hashlib.sha256(tickers[10].encode()).hexdigest() == '104671425b6d8ba2bbf18db03a7144427eff2afce7f5a180b67687ea7160ed2c'\n",
    "assert hashlib.sha256(tickers[79].encode()).hexdigest() == '811ebf6f0e86baf332242b37c11f3bcb8a06ad9b128f137b7ae72d707b43bc2e'\n",
    "assert hashlib.sha256(tickers[263].encode()).hexdigest() == '6da43b944e494e885e69af021f93c6d9331c78aa228084711429160a5bbd15b5'\n",
    "assert hashlib.sha256(' '.join(tickers).encode()).hexdigest() == '604bd5836b606048446b5632c3146d9d8f4d74d514fde285d4ef22599aeca126'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0409818769bbf3d0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q2. Airline tweets\n",
    "\n",
    "Since probably most of us have been missing traveling during this pandemic, we'll be working with some [tweets](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) describing how people felt about certain airlines to remind us of some of the horrors -- and joys -- of air travel.\n",
    "\n",
    "First, we will be performing common preprocessing operations on this text. Start by downloading the data and loading it into a list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-abce95c72c255d16",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"data/Tweets.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df[['airline_sentiment', 'text']]\n",
    "# ignore the neutral class for this exercise\n",
    "df = df[df['airline_sentiment'] != 'neutral']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11541, 2)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['airline_sentiment']\n",
    "\n",
    "# train dev test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8078,), (8078,), (1731,), (1731,), (1732,), (1732,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_dev.shape, y_dev.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-addc0c904c359402",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.a)\n",
    "\n",
    "First tokenize the data. Implement the function to receive an NLTK-style tokenizer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b7930a61806e32",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_tokenizer(data, tokenizer):\n",
    "   \n",
    "    #Returns a list of lists, with the tokens of given text. I.e\n",
    "    #for an input ['Abc def', 'Ghi jkl mn'] it returns [['Abc', 'def'], ['Ghi', 'jkl', 'mn']]\n",
    "    #tokenizer = WordPunctTokenizer()\n",
    "    nltk = []\n",
    "    for sentences in data:\n",
    "        words = tokenizer.tokenize(sentences)\n",
    "        nltk.append(words)\n",
    "    return nltk\n",
    "    #Args:\n",
    "    #data - list with the data\n",
    "    #tokenizer - nltk tokenizer\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f829c5a222c54690",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "data_tok = apply_tokenizer(X_train, tokenizer)\n",
    "\n",
    "assert len(data_tok) == 8078\n",
    "assert len([w for s in data_tok for w in s]) == 191097\n",
    "assert hashlib.sha256(' '.join(data_tok[1234]).encode()).hexdigest() == 'cd12c361e49ccd59ba526ee54b8a6093787977c0ed8bfc43358e5fcf0b0f44c3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b76d100b971a777f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.b)\n",
    "\n",
    "The second step you will implement is lowercasing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee47fb5a45fbd622",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_lowercase(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with all the tokens lowecased.\n",
    "    \n",
    "    Args:\n",
    "    data - list with tokenized data\n",
    "    \"\"\"\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    nltkword = []\n",
    "    nltklis = []\n",
    "    for lis in data:\n",
    "        nltkword=[]\n",
    "        for word in lis:\n",
    "            #if word not in string.punctuation:\n",
    "            word = word.lower()\n",
    "            nltkword.append(word)\n",
    "        nltklis.append(nltkword)    \n",
    "    return nltklis\n",
    "    #return nn\n",
    "    \n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-7979e12840663ea2",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc = apply_lowercase(data_tok)\n",
    "\n",
    "assert len(data_tok_lc) == 8078\n",
    "assert len([w for s in data_tok_lc for w in s]) == 191097\n",
    "assert hashlib.sha256(' '.join(data_tok_lc[1234]).encode()).hexdigest() == 'bc48ec5ed569e79050a6c09c3a9e1e9f7e73d29aaf0c9ef948867d2e31ba25ff'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8044c14c20583cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.c)\n",
    "\n",
    "Now implement a function that filters the stopwords. We will use NLTK's built-in English stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-42ecb29c8fe117f1",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_stopwords(data, stopword_list):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no stopwords.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stopword_list - list of stopwords to filter out\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter the stopwords from the text\n",
    "    # data_filt = ...\n",
    "    # YOUR CODE HERE\n",
    "   \n",
    "    data_filt = []\n",
    "    for lis in data:\n",
    "        nltkword = []\n",
    "        for word in lis:\n",
    "            if word not in stopword_list:\n",
    "                nltkword.append(word)\n",
    "        data_filt.append(nltkword)    \n",
    "    #return nltklis\n",
    "    #raise NotImplementedError()\n",
    "    return data_filt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-f70da0255ea6e291",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc_sw = apply_filter_stopwords(data_tok_lc, stopword_list)\n",
    "assert len(data_tok_lc_sw) == 8078\n",
    "assert len([w for s in data_tok_lc_sw for w in s]) == 124220\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw[1234]).encode()).hexdigest() == 'a281eeb3450057df271950c6c76d287eb6ba05f7a6e9b5c5949e0e6b73e847c7'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d0cb317596faa2f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.d)\n",
    "\n",
    "After filtering stopwords, we want to remove punctuation from the text as well. Make use of `string.punctuation` to do so. Note: your function should only remove tokens that are single punctuation marks. Tokens such as `'!!'` or `'@JetBlue'` should be kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0cd3cf5cc97a8f2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_filter_punct(data):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with no punctuation.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    \"\"\"\n",
    "    data_filt = []\n",
    "    for lis in data:\n",
    "        nltkword = []\n",
    "        for word in lis:\n",
    "            if word not in string.punctuation:\n",
    "                nltkword.append(word)\n",
    "        data_filt.append(nltkword)    \n",
    "    #return nltklis\n",
    "    #raise NotImplementedError()\n",
    "    return data_filt\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-650a677a3a01d1bf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data_tok_lc_sw_punct = apply_filter_punct(data_tok_lc_sw)\n",
    "\n",
    "assert len(data_tok_lc_sw_punct) == 8078\n",
    "assert len([w for s in data_tok_lc_sw_punct for w in s]) == 91017\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw_punct[1234]).encode()).hexdigest() == 'eb94735dd3b8067e44f7529ffd5e358afed116a6acd2fd349e1f67bc770d6ef4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-38d66dd89731e114",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.e)\n",
    "\n",
    "The last preprocessing step you are going to implement is stemming. Implement the function to receive an NLTK-style stemmer and return the token list for each sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a831ba989f3e50e6",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_stemmer(data, stemmer):\n",
    "    \"\"\"\n",
    "    Returns a list of lists, with stemmed data.\n",
    "    \n",
    "    Args:\n",
    "    data - list with the tokenized data\n",
    "    stemmer - instance of stemmer to use\n",
    "    \"\"\"\n",
    "    data_stem = []\n",
    "    for lis in data:\n",
    "        nltkword = []\n",
    "        for word in lis:\n",
    "            stems = stemmer.stem(word)\n",
    "            nltkword.append(stems)\n",
    "        data_stem.append(nltkword)    \n",
    "    #return nltklis\n",
    "    #raise NotImplementedError()\n",
    "    return data_stem\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-3596a6510ebbda3d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "data_tok_lc_sw_punct_stem = apply_stemmer(data_tok_lc_sw_punct, stemmer)\n",
    "\n",
    "assert len(data_tok_lc_sw_punct_stem) == 8078\n",
    "assert len([w for s in data_tok_lc_sw_punct_stem for w in s]) == 91017\n",
    "assert hashlib.sha256(' '.join(data_tok_lc_sw_punct_stem[1234]).encode()).hexdigest() == '248360a1225a1f168e92ca94c5563fb19aec943325dda3ccc6111c1d763f09be'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2bfe5aed6ebdbb26",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q2.f)\n",
    "\n",
    "Finally, join everything in a function, that applies the steps in the following order, in :\n",
    "* Tokenization\n",
    "* Lowercasing\n",
    "* Filtering stopwords\n",
    "* Filtering punctuation\n",
    "* Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ea5b2305431c20dd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, lower=True, remove_punct=True, stopwords=[]):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        self.stopwords = stopwords\n",
    "    \n",
    "    def clean_sentences(self, sentences):\n",
    "                \n",
    "        # Split sentence into list of words\n",
    "        sentences_tokens = apply_tokenizer(sentences, self.tokenizer)    #self.tokenizer.tokenize(sentences)\n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()\n",
    "        \n",
    "        # Lowercase\n",
    "        if self.lower:\n",
    "             sentences_tokens = apply_lowercase(sentences_tokens)    #sentences_tokens.lower()\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "             sentences_tokens = apply_filter_punct(sentences_tokens)#words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        if self.stopwords:\n",
    "            sentences_tokens = apply_filter_stopwords(sentences_tokens, self.stopwords)\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "    \n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            sentences_tokens = apply_stemmer(sentences_tokens, self.stemmer)#map(self.stemmer.stem, sentences_tokens )\n",
    "            # YOUR CODE HERE\n",
    "            #raise NotImplementedError()\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentences_prep = [\" \".join(tokens).strip() for tokens in sentences_tokens]\n",
    "        return sentences_prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a4c1aa16cea2ffb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "text_cleaner = TextCleanerTransformer(\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopword_list\n",
    ")\n",
    "\n",
    "X_train_pre = text_cleaner.clean_sentences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@USAirways It was US 893. The gate was open after about 50 mins waiting. What a great way to finish an 18 hour delayed arrival!!',\n",
       "  'usairway us 893 gate open 50 min wait great way finish 18 hour delay arriv !!'),\n",
       " ('@JetBlue is the greatest airline ever üíï‚úàÔ∏èüí∫ #TrueBluePoints #jetbluemember',\n",
       "  'jetblu greatest airlin ever üíï‚úàÔ∏èüí∫ truebluepoint jetbluememb'),\n",
       " (\"@SouthwestAir is having a sale! I'm delighted!\",\n",
       "  'southwestair sale delight')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(X_train, X_train_pre))[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4a87d0c9b1f20f7e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(X_train_pre) == 8078\n",
    "assert len([w for s in X_train_pre for w in s.split()]) == 91017\n",
    "assert X_train_pre[8] == 'unit travel megzezzo injur gate agent chicago awesom help ty roadwarrior'\n",
    "assert X_train_pre[7999] == 'jetblu case alert arriv late flight four hour delay buy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f48abbdab8acc3d1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Q3. Text classification\n",
    "\n",
    "We will now use what we've learned to try to classify the sentiment of these airline tweets as positive or negative. Let's first load the preprocessed data (it's slightly different from the answer to Q2) and double check the balance of the classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_list(file_name):\n",
    "    with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        return [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = file_to_list('data/tweets_train_preprocessed.txt')\n",
    "X_dev_pre = file_to_list('data/tweets_dev_preprocessed.txt')\n",
    "X_test_pre = file_to_list('data/tweets_test_preprocessed.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    6407\n",
       "positive    1671\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    1380\n",
       "positive     351\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dev.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7e5147de37fa0351",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So, we should be aiming for better than 80% accuracy which is what we would get if we naively predicted negative for everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35b87545f98deeea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.a) \n",
    "The first thing we'll try is the simple baseline of a Bag of Words model. Use sklearn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-046593b38d1d0a86",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<8078x8211 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 82810 stored elements in Compressed Sparse Row format>,\n",
       " <1731x8211 sparse matrix of type '<class 'numpy.int64'>'\n",
       " \twith 16579 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit and transform the preprocessed train and dev data with CountVectorizer\n",
    "#Encode the lables y_train and y_dev\n",
    "vec = CountVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train_pre)\n",
    "X_dev_vec = vec.transform(X_dev_pre)\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()\n",
    "X_train_vec,X_dev_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a4941d45e32e6ca7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert len(vec.vocabulary_) == 8211\n",
    "assert 'awwweesssooome' in vec.vocabulary_\n",
    "assert vec.vocabulary_['awwweesssooome'] == 1544\n",
    "assert hashlib.sha256(' '.join(vec.get_feature_names()[-20:]).encode()).hexdigest() == '585a48103c34d5be5b489f3aef9534bf02dde2e56949e5a48b70e95c5e834304'\n",
    "assert ' '.join([str(i) for i in X_train_vec[11].indices]) == '3601 1313 3798 5322 2651 1991 3355 7285 7042 3991 7050 6701'\n",
    "assert ' '.join([str(i) for i in X_train_vec[11].data]) == '1 1 1 1 1 1 1 1 1 1 1 1'\n",
    "assert ' '.join([str(i) for i in X_dev_vec[1111].indices]) == '1287 1313 1591 1977 2583 3339 3342 4029 4506 4525 4886 5344 8046'\n",
    "assert ' '.join([str(i) for i in X_dev_vec[1111].data]) == '1 1 1 1 1 1 1 1 1 1 1 1 1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b676075741ecae30",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's take a look at some of the words in our vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8652c29d3d784f8c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '000ft', '000lbs', '0016', '00am', '00p', '00pm', '01', '0162389030167']\n",
      "['8wbzorrn3c', '8x7xvm', '90', '900', '900s', '904am', '90min', '910', '912', '9148445695']\n",
      "['mk', 'mke', 'mkpognntyc', 'mkt', 'mktg', 'mkwlkr', 'ml', 'ml1jacpmch', 'mli', 'mmm']\n"
     ]
    }
   ],
   "source": [
    "print(vec.get_feature_names()[:10])\n",
    "print(vec.get_feature_names()[1000:1010])\n",
    "print(vec.get_feature_names()[5000:5010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the words seem pretty random, we might not need them at all. But let's come back to this later and get our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3c28fc309dcd2e90",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# finally, train a Multinomial Naive Bayes classifier on train and predict on dev\n",
    "# store the classifier in a variable clf\n",
    "# return the dev set predictions in a variable y_dev_pred\n",
    "# YOUR CODE HERE\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "#le.fit(y_dev)\n",
    "y_dev = le.transform(y_dev)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec,y_train)\n",
    "# Train the classifier\n",
    "y_dev_predn= clf.predict(X_dev_vec)\n",
    "y_dev_pred = []\n",
    "for num in y_dev_predn:\n",
    "    if num ==1:\n",
    "        pre = 'positive'\n",
    "    else:\n",
    "        pre = 'negative'\n",
    "    y_dev_pred.append(pre)    \n",
    "    \n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-41ffc1ce5d8435d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_allclose(clf.intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative positive negative negative negative negative positive negative negative negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94      1380\n",
      "           1       0.85      0.63      0.72       351\n",
      "\n",
      "    accuracy                           0.90      1731\n",
      "   macro avg       0.88      0.80      0.83      1731\n",
      "weighted avg       0.90      0.90      0.90      1731\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_dev_pred = y_dev_predn\n",
    "# check the results\n",
    "print(classification_report(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: americanair exact ill fli aa dalla airlin trust\n",
      "Predicted: 0, Actual: 1\n",
      "\n",
      "Sentence: delet account jetblu\n",
      "Predicted: 1, Actual: 0\n",
      "\n",
      "Sentence: unit flight rsw tonight amp twin 3 year old pilot row stay help get boy amp bag lifesav\n",
      "Predicted: 0, Actual: 1\n",
      "\n",
      "Sentence: southwestair holi fuckinf shit\n",
      "Predicted: 0, Actual: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we should also look at some misclassified examples\n",
    "for text, pred, true in zip(X_dev_pre[:50], y_dev_pred[:50], y_dev[:50]):\n",
    "    if pred != true:\n",
    "        print(f\"Sentence: {text}\")\n",
    "        print(f\"Predicted: {pred}, Actual: {true}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We beat the naivest baseline of always guessing negative, but let's see if we can do better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.c)\n",
    "Now let's try ngrams instead of plain unigrams to see if we get a boost in performance. But first, let's streamline the process in a nice function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-af90809e6c74e780",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate(X_train, X_dev, y_train, y_dev, ngram_range=(1,1), max_features=None):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with the predictions and the\n",
    "    current accuracy in the validation set. Print the classification report as well.\n",
    "    Assume the documents are already preprocessed\n",
    "    \n",
    "    Args:\n",
    "    X_train - preprocessed tweets in training data\n",
    "    X_dev - preprocessed tweets in dev data\n",
    "    y_train - labels of training data\n",
    "    y_dev - labels of dev data\n",
    "    ngram_range - ngram range to use in CountVectorizer (tuple)\n",
    "    max_features - max number of features to use in CountVectorizer (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline containing the countvectorizer and the multinomial NB classifier\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # y_dev_pred = (...)\n",
    "    # print the classification report\n",
    "    # acc = (...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    text_clf = Pipeline([('vect',CountVectorizer(ngram_range=ngram_range, max_features= max_features)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "    text_clf.fit( X_train, y_train)\n",
    "\n",
    "    y_dev_pred = text_clf.predict( X_dev)\n",
    "#np.mean(predicted == validation_df['sentiment'])\n",
    "    #raise NotImplementedError()\n",
    "    #acc = classification_report(y_dev, y_dev_pred)\n",
    "    acc = np.mean(y_dev_pred == y_dev)\n",
    "    #print(classification_report(y_dev, y_dev_pred))\n",
    "    y_dev_predn=[]\n",
    "    for num in y_dev_pred:\n",
    "        if num ==1:\n",
    "            pre = 'positive'\n",
    "        else:\n",
    "            pre = 'negative'\n",
    "        y_dev_predn.append(pre)   \n",
    "    y_dev_pred = y_dev_predn\n",
    "    \n",
    "    return text_clf,y_dev_pred,acc\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901790872328134"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-97ba1cceed5a53f9",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev)\n",
    "\n",
    "# check same as before\n",
    "assert_allclose(clf['clf'].intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative positive negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.9024, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.d)\n",
    "Now try with both unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e471f692627c2756",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8902368573079145"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run train_and_validate() but with the correct ngram range to have both unigrams and bigrams in the vocabulary\n",
    "# clf, y_dev_pred, acc = ...\n",
    "# YOUR CODE HERE\n",
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev,ngram_range=(1,2), max_features=None)\n",
    "acc\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-865becc9662b6c06",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert_allclose(clf['clf'].intercept_, np.array([-1.57572207]), rtol=1e-3)\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'positive negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.8908, rtol=1e-2)\n",
    "assert len(clf['vect'].vocabulary_) == 60813"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3.e)\n",
    "Find the top 20 most common vocabulary items in the training data. Hint: don't forget how the countvectorizer BoW matrix is actually structured, and how you may have to combine the rows to get the information you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-088364758c0cf246",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# transform the preprocessed training data again\n",
    "# return the a list of tuples containing the top 20 most common ngrams and their counts (ngram, count)\n",
    "#top_20_ngrams = ...\n",
    "stopwords = [line.strip(\"\\n\") for line in open(\"./data/english_stopwords.txt\", \"r\")]\n",
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "    docs = X_train_pre\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in string.punctuation]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "# YOUR CODE HERE\n",
    "top_20_ngrams = list(build_vocabulary().items())[:20]\n",
    "#top_20_ngrams\n",
    "#vec = CountVectorizer(stop_words = 'english')\n",
    "#vec.fit(X_train_pre)\n",
    "#X_train_vec =vec.transform(X_train_pre)\n",
    "#vocabulary = vec.vocabulary_\n",
    "#dict(vocabulary)\n",
    "##vocabulary = vec.vocabulary_\n",
    "# Get the inverse map to map vector indexes to words\n",
    "##inv_map = {v: k for k, v in vocabulary.items()}\n",
    "\n",
    "# Extract the corresponding word and count\n",
    "##counts = [(inv_map[i], X_train_vec[0, i]) for i in columns]\n",
    "\n",
    "##for word, count in counts:\n",
    "   ## print(word, \": \", count)\n",
    "# top_20_ngrams = ...\n",
    "# YOUR CODE HERE\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1e00b9db09c36cc1",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert top_20_ngrams == [('flight', 2844),\n",
    "                         ('unit', 2373),\n",
    "                         ('usairway', 1875),\n",
    "                         ('americanair', 1725),\n",
    "                         ('southwestair', 1236),\n",
    "                         ('jetblu', 1180),\n",
    "                         ('thank', 1060),\n",
    "                         ('get', 975),\n",
    "                         ('hour', 814),\n",
    "                         ('cancel', 678),\n",
    "                         ('delay', 666),\n",
    "                         ('servic', 647),\n",
    "                         ('time', 633),\n",
    "                         ('help', 604),\n",
    "                         ('custom', 600),\n",
    "                         ('call', 518),\n",
    "                         ('wait', 515),\n",
    "                         ('co', 494),\n",
    "                         ('bag', 485),\n",
    "                         ('hold', 475)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9cbc43f83473ed9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### Q3.f)\n",
    "We saw with just unigrams that there were already a lot of unhelpful words in the vocabulary, and now with the addition of bigrams the vocabulary is much bigger. Let's get rid of some infrequent ngrams by limiting the max number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c3e6d0f22dc12fb5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# run train_and_validate() with uni- and bigrams and max features of 20000\n",
    "# clf, y_dev_pred, acc = ...\n",
    "# YOUR CODE HERE\n",
    "clf, y_dev_pred, acc = train_and_validate(X_train_pre, X_dev_pre, y_train, y_dev,ngram_range=(1,2), max_features=20000)\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25d7ead74fe6ec50",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert ' '.join(y_dev_pred[10:20]) == 'negative negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.9076, rtol=1e-2)\n",
    "assert len(clf['vect'].vocabulary_) == 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics improved a little! Let's see if we can get even better performance now by using the relative importance of ngrams with TfIdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28921443e50007c5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4. TfIdf\n",
    "#### Q4.a)\n",
    "First, rewrite the train and validate function from before to include the TfIdf step in the Pipeline. Use sklearn's `TfIdfTransformer`. Also, add kwargs for CountVectorizer's `max_df` and `min_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-02453bb681cd33b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_validate_with_tfidf(X_train, X_dev, y_train, y_dev, ngram_range=(1,1), max_features=None, max_df=1.0, min_df=1):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with the predictions and the\n",
    "    current accuracy in the validation set. Print the classification report as well.\n",
    "    Assume the documents are already preprocessed\n",
    "    \n",
    "    Args:\n",
    "    X_train - preprocessed tweets in training data\n",
    "    X_dev - preprocessed tweets in dev data\n",
    "    y_train - labels of training data\n",
    "    y_dev - labels of dev data\n",
    "    ngram_range - ngram range to use in CountVectorizer (tuple)\n",
    "    max_features - max number of features to use in CountVectorizer (int)\n",
    "    max_df = max_df for CountVectorizer (int or float)\n",
    "    min_df = min_df for CountVectorizer (int or float)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline containing the countvectorizer and the multinomial NB classifier\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # y_dev_pred = (...)\n",
    "    # print the classification report\n",
    "    # acc = (...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range, max_features= max_features,max_df=max_df, min_df=min_df)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "    #Train the classifier\n",
    "    text_clf.fit( X_train, y_train)\n",
    "\n",
    "    y_dev_pred = text_clf.predict( X_dev)\n",
    "#np.mean(predicted == validation_df['sentiment'])\n",
    "    #raise NotImplementedError()\n",
    "    #acc = classification_report(y_dev, y_dev_pred)\n",
    "    acc = np.mean(y_dev_pred == y_dev)\n",
    "    #print(classification_report(y_dev, y_dev_pred))\n",
    "    y_dev_predn=[]\n",
    "    for num in y_dev_pred:\n",
    "        if num ==1:\n",
    "            pre = 'positive'\n",
    "        else:\n",
    "            pre = 'negative'\n",
    "        y_dev_predn.append(pre)   \n",
    "    y_dev_pred = y_dev_predn\n",
    "    \n",
    "    return text_clf,y_dev_pred,acc\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-ee1c189fe642928a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "clf, y_dev_pred, acc = train_and_validate_with_tfidf(X_train_pre, X_dev_pre, y_train, y_dev, max_df=0.95, min_df=0.05)\n",
    "\n",
    "assert ' '.join(y_dev_pred[10:20]) == 'negative negative negative negative negative negative positive negative negative negative'\n",
    "assert_allclose(acc, 0.8492, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4.b)\n",
    "Next, find the top 20 most important words in the training tweets according to TfIdf. The solution should be similar to how you found the most common words from CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f1b44cc6ab17796",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flight', 2844),\n",
       " ('unit', 2373),\n",
       " ('usairway', 1875),\n",
       " ('americanair', 1725),\n",
       " ('southwestair', 1236),\n",
       " ('jetblu', 1180),\n",
       " ('thank', 1060),\n",
       " ('get', 975),\n",
       " ('hour', 814),\n",
       " ('cancel', 678),\n",
       " ('delay', 666),\n",
       " ('servic', 647),\n",
       " ('time', 633),\n",
       " ('help', 604),\n",
       " ('custom', 600),\n",
       " ('2', 544),\n",
       " ('call', 518),\n",
       " ('wait', 515),\n",
       " ('co', 494),\n",
       " ('bag', 485)]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# don't forget to re-transform the preprocessed training data\n",
    "# top_20_most_important_words = ...\n",
    "# do not return the scores, just the words\n",
    "# YOUR CODE HERE\n",
    "stopwords = [line.strip(\"\\n\") for line in open(\"./data/english_stopwords.txt\", \"r\")]\n",
    "def build_vocabulary():\n",
    "    vocabulary = Counter()\n",
    "    docs = X_train_pre\n",
    "    for doc in docs:\n",
    "        words = [word for word in doc.split() if word not in string.punctuation]\n",
    "        vocabulary.update(words)\n",
    "    \n",
    "    return OrderedDict(vocabulary.most_common())\n",
    "\n",
    "# YOUR CODE HERE\n",
    "top_20_ngrams = list(build_vocabulary().items())[:20]\n",
    "top_20_ngrams\n",
    "\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8d60f5f2f2137831",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert top_20_most_important_words == ['unit',\n",
    "                                         'flight',\n",
    "                                         'usairway',\n",
    "                                         'americanair',\n",
    "                                         'southwestair',\n",
    "                                         'jetblu',\n",
    "                                         'thank',\n",
    "                                         'get',\n",
    "                                         'hour',\n",
    "                                         'delay',\n",
    "                                         'time',\n",
    "                                         'servic',\n",
    "                                         'help',\n",
    "                                         'cancel',\n",
    "                                         'custom',\n",
    "                                         'bag',\n",
    "                                         'call',\n",
    "                                         'wait',\n",
    "                                         'plane',\n",
    "                                         'co']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the most important words from TfIdf are slightly different from the most common words from the BoW. The metrics we got using TfIdf may have gotten worse, but with a bit more tuning maybe we can get get better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ab54cc6297c36ee2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### Q4.c)\n",
    "\n",
    "Use the `train_and_validate_with_tfidf` function you created before to train with different hyperparameters and get an accuracy score above 88.5% on the validation dataset. (This threshold is below what we got for plain CountVectorizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-07b6f3694941ac8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# clf, _, acc = train_and_validate_with_tfidf(...)\n",
    "# YOUR CODE HERE\n",
    "clf, y_dev_pred, acc = train_and_validate_with_tfidf(X_train_pre, X_dev_pre, y_train, y_dev,ngram_range=(1,2), max_features=2000, max_df=0.99, min_df=1)\n",
    "acc\n",
    "#raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-07b6f3694941ac81",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert(acc >= 0.885)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate your model on the test set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = clf['tfidf'].transform(clf['vect'].transform(X_test_pre))\n",
    "y_test_pred = clf['clf'].predict(X_test_vec)\n",
    "#print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ended up not being able to beat our baseline of BoW with Tfidf, maybe because the dataset is small and simple, and a simple algorithm was enough. Still, in general it's good to try TfIdf for text classification tasks and have an understanding of how your results change with different hyperparameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
