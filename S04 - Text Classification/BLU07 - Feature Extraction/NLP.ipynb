{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search,findall,finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = re.search(\"someword\", text)\n",
    "match = re.findall('someword',text)\n",
    "match = re.finditer('someword',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following regex will find all whole words start with capital and continuou with lowercase letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall(\"[A-Z][a-z]+\", text)   #if we have multiline text we need to input re.MULTILINE as an arument inside re.findall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can define tokenizer according to the text we have by ourselves\n",
    "tokenizer = RegexpTokenizer('[A-Z]\\w+')\n",
    "# we also can use predefined tukenizer as follows\n",
    "#BlanklineTokenizer - Tokenize a string using blank lines as delimiter.\n",
    "#WordPunctTokenizer - Tokenize a string into alphabetic and non-alphabetic characters.\n",
    "#WhitespaceTokenizer- Tokenize a string using spaces, tabs and newlines as delimiters.\n",
    "from nltk.tokenize import BlanklineTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tokenizer = BlanklineTokenizer\n",
    "#Then we use it to split out text\n",
    "tokens = tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "stems = [list(map(stemmer.stem, words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After tokenization we can use Ngrams.(words is the list of tokenize text or list of words)\n",
    "list(ngrams(words, 1))\n",
    "list(ngrams(words, 2))\n",
    "list(ngrams(words, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "df = pd.read_csv('./data/imdb_sentiment.csv')\n",
    "\n",
    "# Get the text\n",
    "docs = df['text']\n",
    "\n",
    "# Split in train and validation\n",
    "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "###we can read from file and represent it as list as follows\n",
    "#def file_to_list(file_name):\n",
    "    #with open(file_name, 'r', encoding='utf-8') as f:\n",
    "        #return [line.strip() for line in f.readlines()]\n",
    "    \n",
    "#X_train_pre = file_to_list('data/tweets_train_preprocessed.txt')\n",
    "#X_dev_pre = file_to_list('data/tweets_dev_preprocessed.txt')\n",
    "#X_test_pre = file_to_list('data/tweets_test_preprocessed.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to implement sentence cleaning\n",
    "class TextCleanerTransformer(TransformerMixin):\n",
    "    def __init__(self, tokenizer, stemmer, regex_list,\n",
    "                 lower=True, remove_punct=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "        self.regex_list = regex_list\n",
    "        self.lower = lower\n",
    "        self.remove_punct = remove_punct\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        X = list(map(self._clean_sentence, X))\n",
    "        return X\n",
    "    \n",
    "    def _clean_sentence(self, sentence):\n",
    "        \n",
    "        # Replace given regexes\n",
    "        for regex in self.regex_list:\n",
    "            sentence = re.sub(regex[0], regex[1], sentence)\n",
    "            \n",
    "        # lowercase\n",
    "        if self.lower:\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "        # Split sentence into list of words\n",
    "        words = self.tokenizer.tokenize(sentence)\n",
    "            \n",
    "        # Remove punctuation\n",
    "        if self.remove_punct:\n",
    "            words = list(filter(lambda x: x not in string.punctuation, words))\n",
    "\n",
    "        # Stem words\n",
    "        if self.stemmer:\n",
    "            words = map(self.stemmer.stem, words)\n",
    "\n",
    "        # Join list elements into string\n",
    "        sentence = \" \".join(words)\n",
    "        \n",
    "        return sentence\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the data using above class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer and a stemmer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "regex_list = [(\"<[^>]*>\", \"\")\n",
    "             ]\n",
    "\n",
    "#cleaner = TextCleanerTransformer(tokenizer, stemmer, regex_list)\n",
    "#docs = cleaner.transform(train_df.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding the sentiment coulmns or target to 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(train_df['sentiment'].values)\n",
    "\n",
    "train_df['sentiment'] = le.transform(train_df['sentiment'].values)\n",
    "validation_df['sentiment'] = le.transform(validation_df['sentiment'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define pipline and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "text_clf = Pipeline([('prep', TextCleanerTransformer(tokenizer, stemmer, regex_list)),\n",
    "                   ('vect', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "                   ('tfidf', TfidfTransformer()),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "text_clf.fit(map(str, train_df['text'].values), train_df['sentiment'].values)\n",
    "\n",
    "predicted = text_clf.predict(map(str, validation_df['text'].values))\n",
    "np.mean(predicted == validation_df['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check classification result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the results\n",
    "print(classification_report(y_dev, y_dev_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word(BOW) model using sklearn's CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and transform the preprocessed train and dev data with CountVectorizer\n",
    "text_cleaner = TextCleanerTransformer(\n",
    "    tokenizer=tokenizer, \n",
    "    stemmer=stemmer,\n",
    "    lower=True, \n",
    "    remove_punct=True, \n",
    "    stopwords=stopword_list\n",
    ")\n",
    "\n",
    "X_train_pre = text_cleaner.clean_sentences(X_train)\n",
    "X_dev_pre = text_cleaner.clean_sentences(X_dev)\n",
    "#Encode the lables y_train and y_dev\n",
    "vec = CountVectorizer()\n",
    "X_train_vec = vec.fit_transform(X_train_pre)\n",
    "X_dev_vec = vec.transform(X_dev_pre)\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "#le.fit(y_dev)\n",
    "y_dev = le.transform(y_dev)\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_vec,y_train)\n",
    "# predict 0 and 1\n",
    "y_dev_pred= clf.predict(X_dev_vec)\n",
    "#positive negative\n",
    "print(classification_report(y_dev, y_dev_pred))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Word(BOW) model using sklearn's CountVectorizer but using function train and validate the input are preprocessed data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(X_train, X_dev, y_train, y_dev, ngram_range=(1,1), max_features=None):\n",
    "    \"\"\"\n",
    "    Train a model using sklearn's Pipeline and return it along with the predictions and the\n",
    "    current accuracy in the validation set. Print the classification report as well.\n",
    "    Assume the documents are already preprocessed\n",
    "    \n",
    "    Args:\n",
    "    X_train - preprocessed tweets in training data\n",
    "    X_dev - preprocessed tweets in dev data\n",
    "    y_train - labels of training data\n",
    "    y_dev - labels of dev data\n",
    "    ngram_range - ngram range to use in CountVectorizer (tuple)\n",
    "    max_features - max number of features to use in CountVectorizer (int)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the pipeline containing the countvectorizer and the multinomial NB classifier\n",
    "    # text_clf = Pipeline(...)\n",
    "    \n",
    "    # Train the classifier\n",
    "    # (...)\n",
    "\n",
    "    # y_dev_pred = (...)\n",
    "    # print the classification report\n",
    "    # acc = (...)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    text_clf = Pipeline([('vect', CountVectorizer(ngram_range=ngram_range, max_features= max_features)),\n",
    "                   ('clf', MultinomialNB())])\n",
    "# Train the classifier\n",
    "    text_clf.fit( X_train, y_train)\n",
    "\n",
    "    y_dev_pred = text_clf.predict( X_dev)\n",
    "#np.mean(predicted == validation_df['sentiment'])\n",
    "    #raise NotImplementedError()\n",
    "    #acc = classification_report(y_dev, y_dev_pred)\n",
    "    acc = np.mean(y_dev_pred == y_dev)\n",
    "    #print(classification_report(y_dev, y_dev_pred))\n",
    "    y_dev_predn=[]\n",
    "    for num in y_dev_pred:\n",
    "        if num ==1:\n",
    "            pre = 'positive'\n",
    "        else:\n",
    "            pre = 'negative'\n",
    "        y_dev_predn.append(pre)   \n",
    "    y_dev_pred = y_dev_predn\n",
    "    \n",
    "    return text_clf,y_dev_pred,acc\n",
    "    # YOUR CODE HERE\n",
    "    #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
