{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7527be455d372803",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import math\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.svm import SVC\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9bab744c327dc14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q1.\n",
    "\n",
    "It's 2020 and every second you spend in quarantine is driving you crazy. The pandemic would already be enough, but it seems that day in, day out, people are commenting on another disaster or weird event happening around the globe. Well, at least according to twitter. \n",
    "\n",
    "<img src=\"media/biblical.png\" width=\"50%\" />\n",
    "\n",
    "\n",
    "_\"Maybe I shouldn't get my news from twitter...\"_, you wonder.\n",
    "\n",
    "But nah, it's too adictive. So, instead, you set your mind on trying to do a little filtering yourself and detect when  disasters are going on and people are tweeting about it. For that purpose you decide to start building a disaster detector.\n",
    "\n",
    "\n",
    "After a lot of trouble extracting and classifying tweets, you come up with a dataset. \n",
    "\n",
    "\n",
    "Load the dataset and check its structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6273f8aa182f664",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/twitter_disaster.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizer = WordPunctTokenizer()\n",
    "#words = tokenizer.tokenize(df)\n",
    "df.target.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0f698ca70b9e015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.a)\n",
    "\n",
    "You realize, after all that you've learned in the NLP specialization, that it might be worth it to process your text a bit. \n",
    "\n",
    "Implement below a preprocessing function that does the following:\n",
    "\n",
    "- tokenizes the text \n",
    "- lowercases the text \n",
    "- removes punctuation \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1492b0bfab58773c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Returns a dataframe with preprocessed text as a new column\n",
    "    \n",
    "    Parameters:\n",
    "        df: input dataframe, containing a \"text\" column\n",
    "    \n",
    "    Returns:\n",
    "        preprocessed_df: output dataframe, with a new column \"preprocessed_text\" \n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    text_col = df['text'].values\n",
    "    text_lower = [sentences.lower() for sentences in text_col]\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    words = [tokenizer.tokenize(sentences) for sentences in text_lower ]\n",
    "    words_filter_all = []\n",
    "    for lis1 in words:\n",
    "        int_filter_lis = []\n",
    "        for word in lis1:\n",
    "            if word not in string.punctuation:\n",
    "                int_filter_lis.append(word)\n",
    "        words_filter_all.append(int_filter_lis)\n",
    "    sentences = [' '.join(lis1) for lis1 in words_filter_all]\n",
    "    df['preprocessed_text'] = sentences\n",
    "    preprocessed_df = df.copy()\n",
    "\n",
    "    \n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return preprocessed_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-761d12c2e662383e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "preprocessed_df = preprocess_data(df)\n",
    "\n",
    "assert hashlib.sha256(preprocessed_df['text'][200].encode()).hexdigest() == 'cb01387fe41613faa876fb6793179578747693d8dbfe57fd7a808376531f836f'\n",
    "assert hashlib.sha256(preprocessed_df['text'][400].encode()).hexdigest() == '89869900237bf608f53e686bacb698075e76102791c7c7973dd6015730ca271c'\n",
    "assert hashlib.sha256(preprocessed_df['text'][745].encode()).hexdigest() == '8ffa8f4f715c83feb96d9d8b427d90e6732e0e048e8d39b5d2694a797ed76c4e'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-28fc4a869d434823",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "See how the preprocessed data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-73641df7a355d937",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13 000 people receive wildfires evacuation ord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                  preprocessed_text  \n",
       "0       1  our deeds are the reason of this earthquake ma...  \n",
       "1       1              forest fire near la ronge sask canada  \n",
       "2       1  all residents asked to shelter in place are be...  \n",
       "3       1  13 000 people receive wildfires evacuation ord...  \n",
       "4       1  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d9cb713e4ce781f1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "With your preprocessing looking nice and after spending hours just coming up with the dataset, you decide to call it a day and save your data. \n",
    "\n",
    "You save it in a different file (never know when you might need the raw one again) and go to sleep. \n",
    "\n",
    "**Note**: for further exercises you'll be using the pre-saved version of the data we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a27675ff9718a6fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.b)\n",
    "\n",
    "In the morning you go back to your data. You start by loading your preprocessed data and split into a train and test set. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0fb42588a6beea1e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunk</td>\n",
       "      <td>Essex</td>\n",
       "      <td>FOOTBALL IS BACK THIS WEEKEND ITS JUST SUNK IN...</td>\n",
       "      <td>0</td>\n",
       "      <td>football is back this weekend its just sunk in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mass%20murderer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>White people I know you worry tirelessly about...</td>\n",
       "      <td>1</td>\n",
       "      <td>white people i know you worry tirelessly about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>displaced</td>\n",
       "      <td>Pedophile hunting ground</td>\n",
       "      <td>.POTUS #StrategicPatience is a strategy for #G...</td>\n",
       "      <td>1</td>\n",
       "      <td>potus strategicpatience is a strategy for geno...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wrecked</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I wrecked my stomach help</td>\n",
       "      <td>0</td>\n",
       "      <td>i wrecked my stomach help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>curfew</td>\n",
       "      <td>urÌ£nus</td>\n",
       "      <td>It was past curfew\\nand we were at the Grove</td>\n",
       "      <td>0</td>\n",
       "      <td>it was past curfew and we were at the grove</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword                  location  \\\n",
       "0             sunk                     Essex   \n",
       "1  mass%20murderer                       NaN   \n",
       "2        displaced  Pedophile hunting ground   \n",
       "3          wrecked                       NaN   \n",
       "4           curfew                   urÌ£nus   \n",
       "\n",
       "                                                text  target  \\\n",
       "0  FOOTBALL IS BACK THIS WEEKEND ITS JUST SUNK IN...       0   \n",
       "1  White people I know you worry tirelessly about...       1   \n",
       "2  .POTUS #StrategicPatience is a strategy for #G...       1   \n",
       "3                          I wrecked my stomach help       0   \n",
       "4       It was past curfew\\nand we were at the Grove       0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  football is back this weekend its just sunk in...  \n",
       "1  white people i know you worry tirelessly about...  \n",
       "2  potus strategicpatience is a strategy for geno...  \n",
       "3                          i wrecked my stomach help  \n",
       "4        it was past curfew and we were at the grove  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/twitter_disaster_preprocessed.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1186    Rly tragedy in MP: Some live to recount horror...\n",
       "4071    I have been bleeding into this typewriter all ...\n",
       "5461    @J3Lyon I'm going to put the FFVII ones out at...\n",
       "5787    Womens Buckle Casual Stylish Shoulder Handbags...\n",
       "7445    Vladimir Putin Issues Major Warning But Is It ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['target'], test_size=0.3, random_state=42)\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86fc8b7b17496e84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "You decide to move forward with some feature extraction, and go for a TFIDF tokenizer. However, you know that your space might be high-dimensional and you want to prepare for that. \n",
    "\n",
    "Implement a function that receives the number of desired features and returns the vectorized data together with the vectorizer and the space dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-128e829939182a14",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features(X_train, X_test, feature_limit=None):\n",
    "    \"\"\"Converts the provided text training and test data into \n",
    "    TF-IDF feature counts. Additionally, returns the used\n",
    "    vectorizer and the space dimension\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: training data\n",
    "        X_test: test data\n",
    "        feature_limit: maximum number of features to use\n",
    "\n",
    "    Returns:\n",
    "        vectorizer: used tf-idf vectorizer \n",
    "        num_features: vectorizer actual number of features, for sanity check\n",
    "        X_train_vec: processed training features\n",
    "        X_test_vec: processed test features\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,max_features=feature_limit)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    #X_train_vec = vectorizer.transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    num_features = X_train_vec.shape[1]\n",
    "    return vectorizer,num_features,X_train_vec,X_test_vec\n",
    "    #raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1f46a8e42a3d0e27",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[12, :]), 3.08383176160954)\n",
    "assert num_features == 16801\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test, feature_limit=100)\n",
    "assert math.isclose(X_train_vec.todense()[121, 12], 0.6280235685065689)\n",
    "assert num_features == 100\n",
    "\n",
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test, feature_limit=232)\n",
    "assert math.isclose(np.sum(X_train_vec.todense()[300, :]), 2.906725853395745)\n",
    "assert num_features == 232\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5329, 232)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dbf799a0c674b23c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1.c)\n",
    "\n",
    "Let's see what happens when we limit the number of features. What are the features the vectorizer selects?\n",
    "\n",
    "Let's start by applying your function to obtain the vectorizer with 500 features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80540a2ca1335c31",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, num_features, X_train_vec, X_test_vec = extract_features(X_train, X_test, feature_limit=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[  8.80961509,  10.90718459,  10.45857005,  14.63844884,\n",
       "          14.61212414,  16.19363178,  15.96182355,  24.98345458,\n",
       "          12.63098344,   9.32671661,  11.60263646,  49.64248134,\n",
       "          26.04245661,  58.29616168,  12.42587355,  10.81991997,\n",
       "          11.63091021,  11.98223042,  61.02642371,  11.97178871,\n",
       "          13.69557135,  20.39097238,  15.23804818,  72.27325563,\n",
       "          54.1601916 , 195.35897931,   9.11258995,  18.2333878 ,\n",
       "          15.74838176,  12.69691529,  77.39859242,  16.21978363,\n",
       "          21.45044658,  11.13955328,  64.1103516 ,  12.05624504,\n",
       "         102.39816448,  13.40963394,  25.08647102,  14.28220851,\n",
       "          10.84979333,  31.77866055,  13.27227636,  13.70688882,\n",
       "          12.46210578,  15.90307453,  13.12691394,  80.46407773,\n",
       "          18.62851222,  41.52609392,  16.34656402,  22.31106958,\n",
       "          18.9873478 ,   8.8567301 ,  10.80818468,  18.97148263,\n",
       "          11.76125272,  17.25216398,  17.1138502 ,  13.00390253,\n",
       "          37.06994914,  30.30466527,  13.8975628 ,  15.22466117,\n",
       "          19.07378797,  10.12589467,  10.98669455,  13.13541148,\n",
       "          27.38557613,  15.16263709,  32.19842353,  12.38061923,\n",
       "          62.52831258, 100.91735242,  12.18115366,  32.61877234,\n",
       "          11.21388704,  53.76456122,  26.85247806,  10.30320496,\n",
       "          12.32321732,  13.83131137,  14.42007837,   9.64127696,\n",
       "           9.92925224,   8.83960173,  13.70388947,  13.2045875 ,\n",
       "          11.65099102,  21.44537082, 491.06242239,  26.22370655,\n",
       "          13.97191879,  12.81914634,  19.0375276 ,  14.48605884,\n",
       "          14.48614685,  11.71232189,  16.88539446,  22.6928012 ,\n",
       "          12.37329632,  34.61062595,  12.21012049,  17.16166949,\n",
       "          13.97607585,  22.57813826,  10.94168626,  32.66858686,\n",
       "          27.74677099,  19.9540042 ,  17.24905301,  12.6580488 ,\n",
       "           7.8509947 ,  12.62030679,  12.39988458,  12.89268738,\n",
       "          11.49067138,  14.570083  ,  13.88174834,  11.26478204,\n",
       "          14.56731913,  13.62550477,  14.07962819,  10.86307222,\n",
       "          24.35326527,  43.26641663,  12.54341151,  39.9812103 ,\n",
       "          12.48120936,  44.10537806,  23.30805454,  14.23793342,\n",
       "          10.75232515,  11.78274648,  10.38352035,  13.23864182,\n",
       "          12.10857856,  17.94021126,  42.48074079,  14.12961341,\n",
       "          12.94583235,  12.24884254,  11.5408268 ,  13.67471757,\n",
       "          15.85206888,  15.10230372,  15.50083592,  18.48711234,\n",
       "          14.34708796,  15.54518972,  13.8667372 ,  13.00500864,\n",
       "          12.12204892,  16.49276658,  15.28032241,  16.41305417,\n",
       "          14.71293836,  16.79836591,  19.92506739,  14.55523809,\n",
       "          56.1301112 ,  26.56975952,  29.45750641,  17.46174669,\n",
       "          14.11843543,  16.06252719,  17.47594487,  17.61433661,\n",
       "          19.28933643, 151.27609634,  16.88402543,  12.08474732,\n",
       "          13.28950927,  83.88928042,  12.11779045,  12.61528563,\n",
       "          25.69320576,  13.00899738,  51.93373449,  15.9513435 ,\n",
       "          11.8819062 ,  24.17850606,  16.67915904,  24.76808131,\n",
       "          11.84301529,  24.73296448,  27.93496652,  11.9275363 ,\n",
       "          17.00268083,  10.92851298,  18.70771133,  30.09979008,\n",
       "          15.01292145,   9.96363835,  12.50121796,  55.06217659,\n",
       "          73.02286193,  16.1444351 ,  14.09510995,  51.27063201,\n",
       "          13.22136644,  10.51720899,  14.8530385 ,  20.75906462,\n",
       "          30.5089268 ,  25.47222169,  13.02811603,  14.99317705,\n",
       "          16.43755794,  25.42163576,  31.37535125,  14.5332399 ,\n",
       "          23.14896321,  16.78803019,  17.04619272,  10.16175514,\n",
       "          10.0994696 ,  13.33800695,  11.9871936 ,  17.80318435,\n",
       "          10.37416982,  12.70449139,  44.40587652, 475.01277021,\n",
       "         100.89612641,  13.50275068,  48.8738461 ,  20.08206135,\n",
       "         264.01779684,  17.24932432,  14.5292167 ,  16.54755688,\n",
       "          40.81008276,   9.84320997,  10.4383633 , 152.39819058,\n",
       "          11.57851396,  11.50199191,  10.9734765 , 123.67329832,\n",
       "          20.39062747,  17.6538212 ,  70.04799419,   9.9726678 ,\n",
       "          22.65209429,  10.49071028,  26.77564946,  13.3176103 ,\n",
       "          22.62625763,  16.72366758,  14.04575019,   9.74174026,\n",
       "          11.47548614,  25.36875451,  23.67075095,  13.09443991,\n",
       "          63.49180905,  16.65631451,  13.57456889,  19.56763291,\n",
       "          11.41588838,  16.52829418,  20.97130286,  11.07756499,\n",
       "          17.46251305,  12.68603727,   9.70807131,  26.54790394,\n",
       "          10.40122077,  20.72680888,  12.13752598,  26.90931517,\n",
       "          20.68495173,  23.86995866,  24.13128001,  66.43017753,\n",
       "          11.46323183,  15.73719737,  11.13397273,  16.79160427,\n",
       "          10.15978233,  16.00215175,  10.33851029,  42.62868371,\n",
       "           9.35432742,  13.34296869,  11.64315714,  16.04645804,\n",
       "          14.34347624,  13.35176622,  10.52478769, 125.84291049,\n",
       "          13.7268855 ,  14.82380722,  19.58525205,  17.59728315,\n",
       "          59.34566758,  48.89556224,  14.02465132,  15.86977711,\n",
       "          60.60368683,  15.51449625,  57.85117593,  51.5569474 ,\n",
       "          28.88583013,  18.37857515,  14.41394763, 249.26165211,\n",
       "          30.42845125,  12.07108106,  18.59192726,   9.07737107,\n",
       "          20.72364884, 151.79883812,  39.59718701,  21.71608442,\n",
       "          44.68592315,  10.30195432,  24.99207238,  58.02296599,\n",
       "           8.14925125,  39.88229323,  12.9472453 ,  14.44006216,\n",
       "           9.28364802,  44.83272945,   9.86732956,  11.20651316,\n",
       "          17.04590473,  15.71955645,  20.38952678,  14.30996388,\n",
       "          33.56620961,  13.29746222,  11.00496424,  12.48216501,\n",
       "          11.51995986,  11.33188998,  36.50369992,  17.57875183,\n",
       "          10.33709464,  18.55686267,  11.73342818,  19.3762496 ,\n",
       "          17.32210412,  11.61413335,   9.90389733,  14.9432849 ,\n",
       "          15.55232413,  10.23800999,  11.12063462,  18.33647747,\n",
       "          13.72853985,  11.90660206,  30.81101633,  14.90163194,\n",
       "          11.84480059,  12.95259988,  12.15519544,  10.94847506,\n",
       "          11.01328749,  11.48096957,  19.74975716,  13.74591068,\n",
       "          19.32904221,  18.06792571,  17.05163453,  27.39058854,\n",
       "          10.3548271 ,  13.14235914,  14.43110706,  12.55310008,\n",
       "          12.65021007,  19.43714677,  15.54808686,  16.15745297,\n",
       "          13.25129756,  15.53232525,  17.30063964,  14.48068599,\n",
       "          18.12398797,  62.96452286,  30.5502373 ,  10.61629235,\n",
       "          10.59847442,  16.05759624,  16.84589129,  29.3105157 ,\n",
       "          10.37206831,  11.87272476,  36.5465659 ,  12.79070217,\n",
       "          11.08939907,  28.96090189,  13.5034382 ,  12.2235069 ,\n",
       "          13.81231525,  12.97941433,  20.35287886,  15.68329445,\n",
       "          12.43222623,  30.58677692, 109.0423451 , 364.24456517,\n",
       "          23.45111296,  25.94246195,  20.26473602,  41.95317442,\n",
       "          14.05292027,  45.81381132,  10.61286752,  10.0248144 ,\n",
       "          22.14876612,  92.13327504,  17.56829824,  13.0201433 ,\n",
       "          18.26475999,  11.89073563,  28.67652492,  14.41555409,\n",
       "         255.03136927,  23.80495344,  13.47028641,  11.89811662,\n",
       "          21.45044818,  13.12971454,  12.22593505,  10.87300249,\n",
       "          11.28771756,  26.13677904,  11.66065257,  12.14464487,\n",
       "          14.89802277,  10.4951805 ,  16.54373854,  13.07570875,\n",
       "          13.06987627,  25.61274017,  20.24755307,  15.97920002,\n",
       "          11.29621501,  64.6150284 ,  33.01112008,   8.66692985,\n",
       "          19.82508997,  63.25300991,  41.45931538,  12.23781566,\n",
       "          20.56050707,  21.6320116 ,  12.63589537,  77.04978504,\n",
       "          23.15279317,  16.49535633,  11.3910217 ,  20.85240719,\n",
       "          55.27555729,  14.64877796,  13.24847235,  16.17716209,\n",
       "          10.84423801,  15.35078441,  28.51886119,  51.12729269,\n",
       "          48.952048  ,  17.6654429 ,  13.27675853,  15.87643025,\n",
       "          13.0660792 ,  10.79963943,  39.64417277,  10.27174684,\n",
       "          29.61069541,  12.19415688,  21.9946658 ,  54.60975971,\n",
       "          11.45353757,  17.72493324, 108.35062953,  14.61109387,\n",
       "          12.18044064,  13.55825751,  17.58998948,  26.59022673,\n",
       "          31.84729811,  11.23872073,  10.74396144,  16.09159551,\n",
       "          10.55364503,  13.31765362,  24.19862087,  19.21728823,\n",
       "          11.39800125, 141.44273619,  62.4234537 ,  32.27855029,\n",
       "          73.66500549,  18.92853983,  22.13385561,  13.34180336]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5a52052e32c04a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now create a function to obtain the selected features of the fitted vectorizer, sorted by their inverse document frequency (in descending order). In case of tie, this is, if several features have the same IDF score, the features should be sorted by alphabetical order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6657a70e1e5e0486",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams_sorted_by_idf(fitted_vectorizer, top_features=30):\n",
    "    \"\"\"Returns a subset of features of a fitted TfidfVectorizer ordered by \n",
    "    their idf score\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "        top_features: Top features to return \n",
    "    \n",
    "    Returns:\n",
    "        ngrams_sorted (list): The top features of fitted_vectorizer sorted in descending order\n",
    "                              by their idf score. In case of tie, the features should be sorted \n",
    "                              by alphabetical order\n",
    "    \"\"\"\n",
    "# YOUR CODE HERE\n",
    "    fitted_vectorizer_vec = fitted_vectorizer.fit_transform(X_train)\n",
    "    sum_idf = fitted_vectorizer_vec.sum(axis=0)\n",
    "    df = pd.DataFrame(sum_idf.reshape(-1,1), index=fitted_vectorizer.get_feature_names(), columns=[\"tfidf\"]) \n",
    "    df = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "    lis_df = list(df.index.values)\n",
    "    return df\n",
    "#raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf = get_ngrams_sorted_by_idf(vectorizer, top_features=100)\n",
    "len(ddf.tfidf.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in ddf.tfidf.values:\n",
    "    if "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b8188311a1f32f69",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-6b66f777b95b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ngrams_sorted_by_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"aacd834b5cdc64a329e27649143406dd068306542988dfc250d6184745894849\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mhashlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msha256\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_ngrams_sorted_by_idf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhexdigest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"9e96274abcf3df0dfcd14617d6a14931c97fca364083918e38f8cfd20d136f0d\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer, top_features=100)[12].encode()).hexdigest() == \"aacd834b5cdc64a329e27649143406dd068306542988dfc250d6184745894849\"\n",
    "assert hashlib.sha256(get_ngrams_sorted_by_idf(vectorizer, top_features=100)[41].encode()).hexdigest() == \"9e96274abcf3df0dfcd14617d6a14931c97fca364083918e38f8cfd20d136f0d\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-adca65f62fff79d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Check the features with top inverse document frequency in your vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cd01e244d60c3db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#show most specifc 30 features of the 100 selected features\n",
    "get_ngrams_sorted_by_idf(vectorizer, top_features=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.d)\n",
    "\n",
    "Not bad, but you remember learning about Chi squared,  method that will select features by relevance and you decide to compare how effective it is. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f0c66b2edf797342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def extract_features_chi2(X_train, X_test, y_train, feature_limit=100):\n",
    "    \"\"\"Converts the provided text training and test data into \n",
    "    TF-IDF feature counts. Additionally, selects the best features\n",
    "    with chi squared method. Finally, it returns the used\n",
    "    vectorizer and the feature selector\n",
    "    \n",
    "    Parameters:\n",
    "        X_train: training data\n",
    "        X_test: test data\n",
    "        y_train: training labels\n",
    "        feature_limit: maximum number of features to use\n",
    "\n",
    "    Returns:\n",
    "        vectorizer: used tf-idf vectorizer \n",
    "        ch2: used feature selector\n",
    "        X_train_ch2: processed training features\n",
    "        X_test_ch2: processed test features\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0b7f52fc1fdef287",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_ch2, ch2, X_train_ch2, X_test_ch2 = extract_features_chi2(X_train, X_test, y_train)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[12, :]), 0.15232491597861014)\n",
    "assert ch2.k == 100\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, X_test_ch2 = extract_features_chi2(X_train, X_test, y_train, feature_limit=123)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[:, 122]), 36.32984514344287)\n",
    "assert ch2.k == 123\n",
    "\n",
    "vectorizer_ch2, ch2, X_train_ch2, X_test_ch2 = extract_features_chi2(X_train, X_test, y_train, feature_limit=232)\n",
    "assert math.isclose(np.sum(X_train_ch2.todense()[300, :]), 1.1419313656030479)\n",
    "assert ch2.k == 232"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15b6c9135c1144ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Once again, apply the extractor to our data and check the top relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de4f48b0315a5c15",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer_ch2, ch2, X_train_ch2, X_test_ch2 = extract_features_chi2(X_train, X_test, y_train, feature_limit=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8835726672eb231a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now create a function similar to the one before to get the selected features ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a6cbf7d5e2f0f189",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_chi2_ngrams_sorted(fitted_vectorizer, fitted_ch2_selector):\n",
    "    \"\"\"Returns the selected features (ngrams) given a vectorizer and \n",
    "    chi-squared selector\n",
    "    \n",
    "    Parameters:\n",
    "        fitted_vectorizer (TfidfVectorizer): A fitted TfidfVectorizer\n",
    "        fitted_ch2_selector (SelectKBest): A fitted chi squared selector \n",
    "    \n",
    "    Returns:\n",
    "        ngrams_sorted (list): The top features of fitted_vectorizer sorted in ascending order\n",
    "                              by their idf score\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e14aae9835f075c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, check the features selected by chi squared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d281f535669c2f4b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "most_important_features = get_chi2_ngrams_sorted(vectorizer_ch2, ch2)\n",
    "\n",
    "most_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-099ecaf666f3e62e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see how often each of the features selected appears in the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75fa0016fc29f7e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "for feature in most_important_features:\n",
    "    print('Documents that contains the word(s) \"%s\"' % feature)\n",
    "    print('----')\n",
    "    docs = X_train.str.lower().str.contains(feature)\n",
    "    print(str(y_train[docs].value_counts()) + '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1563c9a666e59d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It should be visible that most features selected are linked to the \"disaster\" class (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32139b7e67be8642",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q2.\n",
    "\n",
    "### Time for some modelling\n",
    "\n",
    "You finally decide to put everything together and get your first baseline. \n",
    "\n",
    "First of all, you figure you should use precision and recall to measure your models performance. Even though our data is not inbalanced it can give you more granularity over your target class - predicting a tweet is about disaster - even if a few tweets of the other class get mislabeled.\n",
    "\n",
    "Implement a function to compute precision and recall given your predictions and true labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3f3f4bf976a21164",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_precision_recall(y_test, y_pred):\n",
    "    \"\"\"Returns the precision and recall of the disaster class (label = 1)\n",
    "    \n",
    "    Parameters:\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        y_pred (Series): Predictions corresponding to X_test\n",
    "\n",
    "    Returns:\n",
    "        precision (float): The precision score of the disaster class (1) on the test data\n",
    "        recall (float): The recall score of the disaster class (1) on the test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-b980bb01bd920a45",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "precision, recall = get_precision_recall(pd.Series(np.random.choice([0, 1], size=30)), pd.Series(np.random.choice([0, 1], size=30)))\n",
    "assert precision == 0.4\n",
    "assert recall == 0.375\n",
    "\n",
    "vectorizer, np.random.seed(30)\n",
    "precision, recall = get_precision_recall(pd.Series(np.random.choice([0, 1], size=30)), pd.Series(np.random.choice([0, 1], size=30)))\n",
    "assert precision == 0.45\n",
    "assert recall == 0.5294117647058824\n",
    "\n",
    "vectorizer, np.random.seed(12)\n",
    "precision, recall = get_precision_recall(pd.Series(np.random.choice([0, 1], size=30)), pd.Series(np.random.choice([0, 1], size=30)))\n",
    "assert precision == 0.6\n",
    "assert recall == 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4e733baddb2a1ae3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "Now that your metrics are ready, you jump to the model. \n",
    "\n",
    "As stopwords appeared in the list of features also, you decide to provide an option to remove stop words.\n",
    "\n",
    "Write up a function that receives the train and test data, vectorizes it, selects a given number of features with chi-squared, and finally trains a multinomial Naive Bayes model to classify the tweets. Expose your predictions precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5802f58ce8de2c27",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_chi2_naive_bayes(X_train, y_train, X_test, y_test, max_features=100, stopwords=None):\n",
    "    \"\"\"Returns a fitted TfidfVectorizer, the chi squared selector, a naive bayes classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        max_features (int): maximum number of features to use\n",
    "        stopwords (str): stopwords to use (if None does not use stopwords). eg 'english'\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer, fitted to X_train\n",
    "        ch2 (SelectKBest): SelectKBest with score function chi2 and provided number of features, \n",
    "                           fitted to vectorized X_train\n",
    "        clf (MultinomialNB): MultinomialNB classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vectorizer, ch2, clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-250e86b596a87818",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, ch2, clf, y_pred_200 = \\\n",
    "    train_model_chi2_naive_bayes(X_train, y_train, X_test, y_test, max_features=200, stopwords=\"english\")\n",
    "assert np.mean(y_pred_200) == 0.17075306479859895\n",
    "assert np.sum(y_pred_200) == 390\n",
    "assert ch2.k == 200\n",
    "assert vectorizer.get_feature_names()[7424] == 'independence'\n",
    "\n",
    "\n",
    "vectorizer, ch2, clf, y_pred_500 = \\\n",
    "    train_model_chi2_naive_bayes(X_train, y_train, X_test, y_test, max_features=500, stopwords=\"english\")\n",
    "assert np.mean(y_pred_500) == 0.2591943957968476\n",
    "assert np.sum(y_pred_500) == 592\n",
    "assert ch2.k == 500\n",
    "assert vectorizer.get_feature_names()[7424] == 'independence'\n",
    "\n",
    "\n",
    "vectorizer, ch2, clf, y_pred_1000 = \\\n",
    "    train_model_chi2_naive_bayes(X_train, y_train, X_test, y_test, max_features=1000, stopwords=\"english\")\n",
    "assert np.mean(y_pred_1000) == 0.3021015761821366\n",
    "assert np.sum(y_pred_1000) == 690\n",
    "assert ch2.k == 1000\n",
    "assert vectorizer.get_feature_names()[7424] == 'independence'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8fd1c973791e6f4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now apply your previous precision and recall function to check the metrics of each prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-32e077c8cb1aea12",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precision, recall = get_precision_recall(y_test, y_pred_200)     \n",
    "print(\"\\nPredictions with 200 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_500)      \n",
    "print(\"\\nPredictions with 500 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_1000)     \n",
    "print(\"\\nPredictions with 1000 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d23a6715e3663773",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It seems we have a high precision but a low recall for predicting a disaster. But as you see, as we increase the number of features used we are sacrificing the precision and gaining on recall, probably widening the range of tweets we are classifying as a disaster. \n",
    "\n",
    "This makes sense given that most of the top selected features are linked to the disaster class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9173a0b6dfe2e4d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q3.\n",
    "\n",
    "You are pretty happy with your baseline but you're still using a pretty number of features to get a good precision/recall ratio. You now decide to move on to other techniques, such as SVD and PCA.\n",
    "\n",
    "\n",
    "Start by writting up a similar function to the previous one to train your model, but use SVD and classify your model with Support Vector Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-43a51c434987d044",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_svd_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Returns a fitted TfidfVectorizer, the truncated svd used, a support vector classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        num_features (int): maximum number of features to use\n",
    "        seed (int): Seed to use for random state\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer, fitted to X_train\n",
    "        svd (TruncatedSVD): TruncatedSVD with provided number of features as components\n",
    "        clf (SVC): SVC classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vectorizer, svd, clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-20f21581792161ac",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, svd, clf, y_pred_30 = train_model_svd_svm(X_train, y_train, X_test, y_test, num_features=30, seed=42)\n",
    "print(\"----- Computed for 30 features -----\\n\")\n",
    "assert np.mean(y_pred_30) == 0.30122591943957966\n",
    "assert np.sum(y_pred_30) == 688\n",
    "assert svd.n_components == 30\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'\n",
    "\n",
    "\n",
    "vectorizer, svd, clf, y_pred_50 = train_model_svd_svm(X_train, y_train, X_test, y_test, num_features=50, seed=42)\n",
    "print(\"----- Computed for 50 features -----\\n\")\n",
    "assert np.mean(y_pred_50) == 0.3261821366024518\n",
    "assert np.sum(y_pred_50) == 745\n",
    "assert svd.n_components == 50\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'\n",
    "\n",
    "vectorizer, svd, clf, y_pred_100 = train_model_svd_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42)\n",
    "print(\"----- Computed for 100 features -----\\n\")\n",
    "assert np.mean(y_pred_100) == 0.3445709281961471\n",
    "assert np.sum(y_pred_100) == 787\n",
    "assert svd.n_components == 100\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-600f1f3be994d779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can obtain the precision and recall as before and compare with the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e0298dc6a13452a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precision, recall = get_precision_recall(y_test, y_pred_30)     \n",
    "print(\"\\nPredictions with 30 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_50)      \n",
    "print(\"\\nPredictions with 50 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_100)     \n",
    "print(\"\\nPredictions with 100 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8be788c4e7462464",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q4.\n",
    "\n",
    "With much less features we get a better recall and only slightly lower precision, nice! Remember now some of the pros and cons of SVD and other dimensionality reduction techniques. While SVD performed better, likely because in theory it can capture information from more than 50 ngrams (whereas in chi-squared the number of features was in a direct correspondence with the number of ngrams), with SVD we lose interpretability and can't examine the most important features!\n",
    "\n",
    "Now let's try PCA.\n",
    "\n",
    "Write a similar function to Q3 but now using PCA to see how it compares to SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c323629aaea537dd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42):\n",
    "    \"\"\"Returns a fitted TfidfVectorizer, the truncated svd used, a support vector classifier\n",
    "    and the test predictions computed with these\n",
    "    \n",
    "    Parameters:\n",
    "        X_train (Series): Text data for training\n",
    "        y_train (Series): Labels corresponding to X_train\n",
    "        X_test (Series): Text data for testing\n",
    "        y_test (Series): Labels corresponding to X_test\n",
    "        num_features (int): maximum number of features to use\n",
    "        seed (int): Seed to use for random state\n",
    "\n",
    "    Returns:\n",
    "        vectorizer (TfidfVectorizer): TfidfVectorizer, fitted to X_train\n",
    "        pca (PCA): PCA with provided number of features as components\n",
    "        clf (SVC): SVC classifier fitted to the feature-selected training data\n",
    "        y_pred (Series): The predictions computed with our classifier\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return vectorizer, pca, clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e0c931cb07614c23",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vectorizer, pca, clf, y_pred_30 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=30, seed=42)\n",
    "print(\"----- Computed for 30 features -----\\n\")\n",
    "assert np.mean(y_pred_30) == 0.30035026269702275\n",
    "assert np.sum(y_pred_30) == 686\n",
    "assert pca.n_components == 30\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_50 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=50, seed=42)\n",
    "print(\"----- Computed for 50 features -----\\n\")\n",
    "assert np.mean(y_pred_50) == 0.3126094570928196\n",
    "assert np.sum(y_pred_50) == 714\n",
    "assert pca.n_components == 50\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'\n",
    "\n",
    "vectorizer, pca, clf, y_pred_100 = train_model_pca_svm(X_train, y_train, X_test, y_test, num_features=100, seed=42)\n",
    "print(\"----- Computed for 100 features -----\\n\")\n",
    "assert np.mean(y_pred_100) == 0.33887915936952717\n",
    "assert np.sum(y_pred_100) == 774\n",
    "assert pca.n_components == 100\n",
    "assert vectorizer.get_feature_names()[5424] == 'exploit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d1ecb5e0f82ec689",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can obtain the precision and recall as before and compare with the previous results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4362d8b3356bd257",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "precision, recall = get_precision_recall(y_test, y_pred_30)     \n",
    "print(\"\\nPredictions with 30 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_50)      \n",
    "print(\"\\nPredictions with 50 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))\n",
    "\n",
    "precision, recall = get_precision_recall(y_test, y_pred_100)     \n",
    "print(\"\\nPredictions with 100 features: \")\n",
    "print(\"Precision: {}\".format(precision))\n",
    "print(\"Recall: {}\".format(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5b17e715e9161796",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Q5.\n",
    "\n",
    "Now we'll change gears a bit and look into word vectors. In Learning Notebook 3 we told you that word vectors can be visualized after being projected into 2D space, and we showed you this diagram:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n",
    "\n",
    "Now we'll try to combine what you've learned about word embeddings and PCA to make our own visualization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3847eb100057a7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3b982ddaa87eec51",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.a)\n",
    "\n",
    "First, to get comfortable with spacy, get the vector for the word \"disaster\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-34ae448736fdde6b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# disaster_vector = \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5cdd4b59c690522a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(disaster_vector.sum(), -6.004221, abs_tol=0.00001)\n",
    "assert disaster_vector.shape[0] == 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7566702b1fd9755",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.b) \n",
    "\n",
    "Next, write a function that uses sklearn's PCA to reduce our vectors to a convenient number of dimensions for plotting. The function should return the reduced dimension word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8c4ffc6d44920880",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def reduce_word_vecs(vectors, random_state):\n",
    "    \"\"\"\n",
    "    Returns PCA-reduced word vectors of the input vectors for plotting\n",
    "    \n",
    "    Parameters:\n",
    "        vectors (np.array): Word vectors to be reduced\n",
    "        random_state (int): random state to use in PCA\n",
    "\n",
    "    Returns:\n",
    "        reduced_vecs (np.array): Word vectors reduced to the number of dimensions\n",
    "                                 suitable for plotting with given random_state\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return reduced_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-05bdaa52733db9e2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_vectors = np.array([[0.1, 0.2, 0.3, 0.4], [0.3, 0.5, 0.1, 0.7], [0.8, 0.6, 0.2, 0.4]])\n",
    "reduced_vecs = reduce_word_vecs(test_vectors, random_state=42)\n",
    "\n",
    "assert reduced_vecs.shape == (3,2)\n",
    "assert math.isclose(reduced_vecs[1][1], 0.24736592153367926)\n",
    "assert math.isclose(reduced_vecs[2][0], 0.43388622222454437)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1f75d7362f3b5b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we'll create an array of ~75,000 of spacy's word vectors and use your PCA function to reduce them. If you're curious about using the full amount of word vectors, you can change the code to iterate over all vocab words - `list(nlp.vocab.strings)` - instead of our own `vocab_strings`, but beware it will use a lot of memory!\n",
    "\n",
    "We'll also set a list of words that we'll plot later and force our set of vectors to include these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d7c693a88480b94",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "words_to_plot = ['banana', 'pineapple', 'mango', 'red', 'blue', 'yellow', 'woman', 'man', 'child', 'playing',\n",
    "                 'reading', 'studying', 'nintendo', 'sony', 'xbox', 'sad', 'angry', 'bored']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-644c6807023bc77f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "with open('datasets/word_subset.txt') as fwords:\n",
    "    vocab_strings = fwords.read().splitlines()\n",
    "\n",
    "full_vocab_vecs = []\n",
    "for tok in vocab_strings:\n",
    "    full_vocab_vecs.append(nlp.vocab.get_vector(tok))\n",
    "\n",
    "vocab_array = np.array(full_vocab_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Word vectors shape pre-PCA: {}'.format(vocab_array.shape))\n",
    "\n",
    "full_vocab_reduced = reduce_word_vecs(vocab_array, random_state=42)\n",
    "\n",
    "print('Word vectors shape after PCA: {}'.format(full_vocab_reduced.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-93396749fc10fb39",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert math.isclose(full_vocab_reduced[100][0], -3.0880196, abs_tol=0.00001)\n",
    "assert math.isclose(full_vocab_reduced[9999][0], -0.9480563, abs_tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0feaee69a29da92",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.c)\n",
    "\n",
    "Time to plot! For this, we'll limit the visualized words to a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9415c85fbda3dbb5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "coords = []\n",
    "for word in words_to_plot:\n",
    "    idx = vocab_strings.index(word)\n",
    "    coords.append(full_vocab_reduced[idx])\n",
    "\n",
    "coords_array = np.array(coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7c059bc4a449b27",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(num=None, figsize=(8, 8), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "plt.xlim(min([x for x in coords_array[:,0]]), max([x for x in coords_array[:,0]]))\n",
    "plt.ylim(min([y for y in coords_array[:,1]]), max([y for y in coords_array[:,1]]))\n",
    "plt.scatter(coords_array[:,0], coords_array[:,1])\n",
    "\n",
    "for item, x, y in zip(words_to_plot, coords_array[:,0], coords_array[:,1]):\n",
    "    plt.annotate(item, xy=(x, y), xytext=(-2, 2), textcoords='offset points', \n",
    "                 ha='right', va='bottom', color='purple', fontsize=14 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dcc429eed65a7f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The PCA seems to have worked! In the diagram we can see similar types of words closer together. But of course, take these visualizations with a grain of salt because it is practically impossible to preserve all distances in a high dimensional space in just 2 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6a980c450673e4fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q5.d) \n",
    "\n",
    "As a final exercise, we'll look at some word similarities.\n",
    "\n",
    "Write a function that returns the next closest word in terms of cosine similarity to a given word. If there are multiple words with the same highest similarity, return all of them.\n",
    "\n",
    "Hint: you can use the already-imported `cosine_similarity` function from sklearn to compute cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-72bc3a66c2a8f550",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def closest_word(input_word, words_in_vocab, word_vectors):\n",
    "    '''Returns a list of the closest word or words to the input word, based on cosine similarities\n",
    "       of the word vectors given\n",
    "    \n",
    "    Parameters:\n",
    "        input_word (string): Search for the closest words to this word\n",
    "        words_in_vocab (list): Vocabulary associated with the vectors in word_vectors\n",
    "        word_vectors (np.array): Word vectors associated with the strings in words_in_vocab\n",
    "\n",
    "    Returns:\n",
    "        closest_words_list (list): List of strings containing the closest word or words to\n",
    "                                   input_word, based on cosine similarities of the word_vectors\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return closest_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d06a90ba7d38c797",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(hashlib.sha256(closest_word('nintendo', words_to_plot, coords_array)[0].encode()).hexdigest() == \"f711da60664c04c146d7a47b722c38a8d0bf46c3f52c2084c5c8d1cb78138e73\")\n",
    "assert(hashlib.sha256(closest_word('playing', words_to_plot, coords_array)[0].encode()).hexdigest() == \"435c149cbc6a5e5cc373cd33347d4c336a22160e06b7df61092b66e56f4d55ec\")\n",
    "assert(hashlib.sha256(closest_word('pineapple', words_to_plot, coords_array)[0].encode()).hexdigest() == \"6815f3c300383519de8e437497e2c3e97852fe8d717a5419d5aafb00cb43c494\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClosest words to nintendo:\")\n",
    "print(closest_word('nintendo', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to playing:\")\n",
    "print(closest_word('playing', words_to_plot, coords_array))\n",
    "\n",
    "print(\"\\nClosest words to pineapple:\")\n",
    "print(closest_word('pineapple', words_to_plot, coords_array))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
