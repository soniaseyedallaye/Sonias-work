{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Word Vectors\n",
    "\n",
    "So far we've seen simple feature selection methods, a statistical feature selection approach, dimensionality reduction techniques such as PCA and SVD, but in the last few years, with the rise in popularity of Neural Networks, a new technique has become the state of the art for representing words in NLP tasks.\n",
    "\n",
    "This technique is commonly referred to as word vectors or word embeddings, and its inner workings are really simple. It consists of defining a vocabulary and a vector for each word in it with a maximum number of dimensions. Then all the vectors' weights are found through the use of neural networks. In essence, word embeddings try to capture information a word's meaning and usage. This not only allows us to reduce significantly the number of features inputed to our models, but it also allows meaningful and easy representations across the data, that are transferrable among tasks. \n",
    "\n",
    "Pretty cool, huh?\n",
    "\n",
    "<img src=\"./media/what-year-is-this.jpg\" width=\"400\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Word Vectors Explained\n",
    "\n",
    "First of all, by now you could be thinking: \"But wait Doc, didn't I get a bunch of vectors before also?\". Why yes, yes you did Marty. You could consider the matrix with document-term counts to contain in their columns a possible word vector representation. You could even construct a simpler matrix. \n",
    "\n",
    "If you assume your vocabulary with size V, and each word having an index in this vocabulary, a natural representation would be what it is called a 1-hot encoding, where each word is represented by a vector of size V - the vocabulary size - with the single component corresponding to its word set to 1, and the remaining zeroed out.\n",
    "\n",
    "<img src=\"./media/one-hot-vec.png\" width=\"300\">\n",
    "\n",
    "\n",
    "We are going in the right direction! But keep in mind that this representation fits in a very large space and we suddenly fall into the pitfalls of high-dimensionality. You could think of applying PCA or SVD to these 1-hot vectors but as for most tasks nowadays, neural networks have proven to be better at the task. To simply put it, here is a more elegant way. \n",
    "\n",
    "<img src=\"./media/but-how-doc.jpg\" width=\"450\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Word Vectors\n",
    "\n",
    "So you know the data - a bunch of words. You know the goal - a vector with an arbitrary number of K features. And you know the means - neural networks. So how does it all work? \"You shall know a word by the company it keeps\". These are the words of John Rupert Firth (at least according to wikipedia), and they are the basis of the following method - Word2vec. \n",
    "\n",
    "**Word2vec** is a popular technique for using neural models to produce word embeddings, and it encompasses two main approaches - Continuous Bag Of Words (CBOW) and skipgram - that we will describe here.\n",
    "\n",
    "Initially, we prepare the dataset to consider for each sentence several windows of length n, centered around each word. Each of these will create training examples that we will plug into our neural network, in one of two ways:\n",
    "\n",
    "1 - **CBOW**: the input words are the context words, and we predict the center word, this is, our model output. \n",
    "\n",
    "2 - **Skip-gram**: complementary to the previous method, the input is the center word, and the predictions are the context words\n",
    "\n",
    "The weights of the network are shared in both cases for the side that has more than one word, and there are a few more details on how setup these models, but the basic intuition can be seen on the following image:\n",
    "\n",
    "<img src=\"./media/word2vec.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained word vectors\n",
    "\n",
    "The best thing about these vectors, however, is that we can transfer them among tasks. What this means is that we don't need to go through the painful task of training them, and we can rely on pretrained vectors. Most of these pretrained vectors were trained on a huge amount of data in the same language, that would take time to gather, process and iterate over to train the network.\n",
    "\n",
    "One set of such pretrained vectors are from **spacy**. [Spacy](https://spacy.io) is a toolkit similar to NLTK, but it contains embedded deep learning models for NLP and it typically has better performance for industrial applications. The pretrained word vectors are easy to use out of the box by importing the spacy library. At this point, if you did not go through the README carefully, you should run this command to download the required models:\n",
    "\n",
    "`python -m spacy download en_core_web_md`\n",
    "\n",
    "Spacy has different versions with different sizes, and the one we are downloading is the medium one. You can try to switch between versions to see the impact it gets in the following experiments. Different sizes are related to different vocabulary sizes and feature size. Load the medium pretrained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.8 MB 4.4 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from en_core_web_md==2.3.1) (2.3.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.19.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.50.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (39.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/sonia/.virtualenvs/blu08/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.3.0)\n",
      "Using legacy 'setup.py install' for en-core-web-md, since package 'wheel' is not installed.\n",
      "Installing collected packages: en-core-web-md\n",
      "    Running setup.py install for en-core-web-md ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-md-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/sonia/.virtualenvs/blu08/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several available libraries of word vectors out there, such as [FastText](https://fasttext.cc) and [Glove](https://nlp.stanford.edu/projects/glove/). These all provide good quality embeddings for your NLP tasks. Their training methods are usually based on the Word2vec, but they normally have a few difference in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Word Representations in Spacy\n",
    "\n",
    "Now let's dig into the vectors and see what we can get from them. We can start by seing the representation for a particular word, for example *house*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! explorer.exe .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.9847e-01,  1.8087e-01, -8.9119e-02, -2.5626e-01,  7.4104e-02,\n",
       "        5.9422e-03, -8.0814e-02, -8.7499e-01,  1.6353e-01,  2.7836e+00,\n",
       "       -8.9134e-01,  3.7017e-02, -5.5995e-01, -2.1853e-01, -3.6847e-01,\n",
       "        4.2609e-01,  2.5508e-02,  1.1834e+00, -5.9869e-02, -1.6261e-02,\n",
       "        3.6331e-01,  1.2664e-01,  3.1424e-01,  2.3845e-02,  5.7331e-02,\n",
       "       -4.7905e-01, -2.3247e-01,  2.3379e-02, -2.9739e-01,  1.0735e-01,\n",
       "        2.9723e-01,  5.4123e-02, -2.6837e-01,  4.8272e-01, -4.8055e-02,\n",
       "       -1.0766e-02,  1.6169e-01, -7.4395e-02,  1.2789e-03, -6.1155e-02,\n",
       "        2.4258e-01,  1.4165e-02,  8.3789e-02, -3.5793e-01, -4.8655e-02,\n",
       "        1.1436e-01,  2.7535e-01, -9.2720e-01,  3.2332e-01,  1.6197e-01,\n",
       "       -2.6260e-01, -3.2542e-01,  1.8347e-01,  5.7849e-01,  1.9925e-01,\n",
       "       -3.7611e-01,  1.8520e-01,  1.3349e-01,  1.9571e-01,  5.1844e-01,\n",
       "        2.0733e-01,  2.0470e-01,  8.3850e-02,  4.2725e-01,  1.1571e-01,\n",
       "       -1.2066e-01, -7.6344e-02,  2.2959e-01, -1.9066e-01,  2.8804e-01,\n",
       "       -4.8705e-02, -1.6430e-01, -9.8883e-02, -3.7394e-01,  3.3152e-02,\n",
       "        4.5618e-02,  2.8564e-01, -3.3728e-01, -2.8675e-01, -2.8868e-01,\n",
       "       -1.8131e-01,  6.1240e-01,  1.6237e-01, -2.2660e-01, -3.7512e-02,\n",
       "        2.1622e-01,  4.1143e-01,  4.1526e-01,  4.8579e-01,  3.4196e-02,\n",
       "        1.1097e-02, -4.2380e-02, -5.8883e-02, -2.5180e-01, -1.1827e-01,\n",
       "       -6.5252e-02,  4.3805e-01,  4.2373e-01, -3.5036e-01,  2.0297e-01,\n",
       "       -9.6745e-02,  5.7277e-01, -1.0347e-01,  3.8731e-02,  1.0371e-01,\n",
       "       -4.1211e-01,  1.6263e-01,  9.9873e-02, -1.9250e-02, -3.4607e-01,\n",
       "       -1.3904e-01,  9.3622e-02,  1.1864e-01, -1.5903e-02,  4.8909e-01,\n",
       "        2.2709e-01, -9.8584e-02,  2.4182e-01,  2.1903e-02,  2.8252e-01,\n",
       "       -2.1217e-01,  2.7143e-01, -2.4541e-01, -5.2238e-01,  2.2886e-01,\n",
       "       -2.6915e-01, -7.0856e-02, -1.4449e-01,  1.7106e-01,  4.7452e-02,\n",
       "       -1.5870e-01,  1.8143e-01, -1.6997e-02, -1.5448e-01,  4.7205e-01,\n",
       "        3.1468e-01,  2.3606e-01,  3.5544e-01,  4.6862e-01, -1.5596e-01,\n",
       "       -1.7340e+00, -1.9391e-01, -2.3963e-02, -3.6020e-01,  4.6535e-02,\n",
       "        2.5813e-01, -9.3215e-02,  2.6950e-01,  2.6694e-01, -1.5030e-01,\n",
       "       -2.0352e-02,  1.8663e-01, -1.9440e-01,  2.3475e-01,  3.4072e-01,\n",
       "       -7.1621e-02,  2.2388e-01, -2.3374e-01,  1.4200e-02, -1.4635e-01,\n",
       "        6.6203e-02, -1.3022e-01, -1.9029e-01,  2.0700e-01,  2.0488e-01,\n",
       "       -5.5457e-01,  3.3125e-01,  1.8895e-01,  6.0863e-01,  2.4981e-01,\n",
       "       -2.1294e-01,  2.0438e-01, -2.1388e-01, -9.2709e-03, -3.2134e-01,\n",
       "       -2.7341e-01, -5.3715e-02,  4.4432e-01, -5.6145e-02,  1.9701e-01,\n",
       "        4.9993e-01, -7.0837e-01,  1.6883e-01, -2.1141e-01, -9.5209e-02,\n",
       "       -2.9795e-02, -7.8936e-02, -2.7381e-01,  3.5855e-01, -1.7869e-01,\n",
       "       -4.1047e-01,  4.8600e-02,  2.7462e-02,  1.1432e-01, -7.6645e-02,\n",
       "        3.2180e-01, -2.4885e-01, -4.2627e-01, -1.6614e-01,  1.6740e-01,\n",
       "       -2.1145e-01,  1.7912e-01,  3.7599e-01, -5.1378e-01,  7.2899e-02,\n",
       "       -1.3659e-01,  1.1925e-01,  3.0539e-02,  2.2776e-01, -2.6466e-01,\n",
       "       -2.8589e-01,  2.8825e-02,  2.5696e-01, -4.6584e-02,  2.1268e-01,\n",
       "       -2.8677e-01,  2.7728e-01, -5.6491e-02, -1.7809e-01,  3.4237e-01,\n",
       "       -5.7061e-02, -6.0279e-02, -1.2577e-01,  1.3695e-01,  1.9769e-01,\n",
       "        1.6630e-01, -2.5674e-01,  1.7000e-01, -3.5881e-01, -3.2292e-01,\n",
       "        3.8045e-01,  6.2803e-02, -2.2209e-01,  2.9701e-01,  5.6837e-02,\n",
       "       -2.7707e-01,  1.1020e-01, -3.1815e-01, -4.6311e-02, -2.8384e-01,\n",
       "       -3.4146e-01,  1.1606e-01,  4.3806e-02,  5.8888e-01, -2.2216e-01,\n",
       "       -2.2103e-01,  3.6901e-01, -5.0477e-01, -1.3206e-01, -5.7208e-01,\n",
       "       -2.5279e-01,  5.1948e-02,  4.1786e-01, -1.3912e-01,  1.7719e-01,\n",
       "        4.1387e-01,  3.6797e-01, -2.6381e-01,  1.4578e-01, -3.0316e-01,\n",
       "       -9.4649e-02,  6.3837e-02,  1.3826e-01, -1.8213e-01,  1.7888e-01,\n",
       "       -1.8555e-01, -1.9941e-01, -1.5132e-01, -1.1393e+00,  1.5898e-01,\n",
       "       -2.9225e-01,  2.0079e-01, -4.9275e-02, -8.0235e-01,  1.2834e-02,\n",
       "       -8.9354e-02, -3.0374e-01,  5.6119e-01, -2.0220e-02,  2.9735e-02,\n",
       "        5.8468e-01, -1.0082e-01, -4.7442e-01, -1.2492e-03, -1.7756e-01,\n",
       "        3.0101e-01,  5.8639e-01,  1.2706e-01, -4.8098e-01, -9.8582e-02,\n",
       "       -2.7866e-01, -3.8891e-01,  5.3706e-02,  7.9971e-01, -3.8533e-01,\n",
       "        2.8433e-01,  3.5182e-02, -2.4263e-01, -3.5183e-02, -2.9661e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp('house').vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a simple function just to make it easier and avoid rewriting the same thing over and over again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also check the size of the vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp('house').vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These word embeddings are 300-dimensional, or, in other words, they have 300 features. We'll come back to this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Cosine similarity\n",
    "\n",
    "We can check similarities between words using cosine similarity. The cosine similarity is a measure of distance between to vectors. It is defined by the following equation:\n",
    "\n",
    "$$\\text{cos-similarity} = \\frac{A \\cdot B}{\\| A \\| \\| B \\|}$$\n",
    "\n",
    "And it's computation is very intuitive in the 2D plane. \n",
    "\n",
    "<img src=\"./media/cosine.png\" width=\"400\">\n",
    "\n",
    "In this example, there are three animals that have two features that represent them - if the animal lives in the woods and how much it hunts. The vectors represent where each animal is in this feature space and so if the vectors are more close together, they are more similar. This can be measured by the cosine of the angle between them - if the angle between two vectors is low (similar vectors), the cosine of that angle is greater and thus the similarity between the words in this feature space is greater!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "def cosine(v1, v2):\n",
    "    if norm(v1) > 0 and norm(v2) > 0:\n",
    "        return dot(v1, v2) / (norm(v1) * norm(v2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it out. Using cosine similarity, closer words - like *house* and *home* - should have higher scores. On the other hand words with different meanings, even if they are close in terms of characters - like *house* and *mouse* - should produce a low score, if our word vectors really capture meaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('house'), vec('home'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine(vec('house'), vec('mouse'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, *house* is closer to *home* than it is to *mouse*. Makes sense!\n",
    "\n",
    "<img src=\"./media/future.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Once again, to simplify our next examples, let's create a function that gets us the closest words to the vector that we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10, dont_include_list=[]):\n",
    "    return sorted([(x, cosine(vec_to_check, vec(x))) for x in token_list if x not in dont_include_list],\n",
    "                  key=lambda x: x[1],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to apply this function in further examples. To simplify a bit, let's limit the vocabulary to the one from our previous example. We can then find the closest words to the word *house*.  We start by reading the dataset and getting its vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/twitter_rep_dem_data_small.csv')\n",
    "\n",
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit_transform(df.Tweet)\n",
    "\n",
    "tweet_vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also obtain the 10 closest words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              vec('house'),\n",
    "              dont_include_list=['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Word relations\n",
    "\n",
    "There are much more that we can do to show you that these vectors capture the meaning, or at least some semantic information, of our vocabulary. Hopefully, if you still don't believe it, this will help. For example, what do you think will happen if we subtract man from king and add woman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('king') - vec('man') + vec('woman'),\n",
    "              dont_include_list=['king', 'man', 'woman'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-2.png\" width=\"300\">\n",
    "\n",
    "\n",
    "And what is the mean between morning and evening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(),\n",
    "              np.mean(np.array([vec('morning'), vec('evening')]), axis=0),\n",
    "              dont_include_list=['morning', 'evening'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-3.png\" width=\"300\">\n",
    "\n",
    "\n",
    "If sky is to blue, grass is to ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_closest(tweet_vocab.keys(), \n",
    "              vec('blue') - vec('sky') + vec('grass'),\n",
    "              dont_include_list=['blue', 'sky', 'grass'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./media/mind-blown-4.png\" width=\"300\">\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5 - Applying word vectors to sentences\n",
    "\n",
    "There are several ways you could think of to construct a sentence representation from these vectors, such as:\n",
    "\n",
    "* sum\n",
    "* average \n",
    "* concatenation\n",
    "\n",
    "The average is a good enough approach to start with, so let's implement a function to get the sentence vector representation from the average of its words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentvec(s):\n",
    "    sent = nlp(s)\n",
    "    return np.mean(np.array([w.vector for w in sent]), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the same logic to get the closest sentence according to the sentence representation we chose. Below you have the implementation of the previous function that used cosine similarity, but for sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest_sent(space, input_str, n=10):\n",
    "    input_vec = sentvec(input_str)\n",
    "    return sorted(space,\n",
    "                  key=lambda x: cosine(sentvec(x), input_vec),\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out with a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in spacy_closest_sent(df.Tweet.values[:2000], \"i am against the trump administration .\"):\n",
    "    print(sent)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have worked quite well, wouldn't you agree, Marty?\n",
    "\n",
    "If you are still not convinced about this, you can try to project all your vectors into a 2D space (by applying PCA, for example) and convince yourself that words are somewhat organized by meaning, and we can extract word relations from its distances. If you project your vectors, you should get something similar to this:\n",
    "\n",
    "<img src=\"./media/word-vectors-projection.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - NLP practical example\n",
    "\n",
    "All that is left is to try to use these new representations as the features of our models. We start by defining a function to build our vectors for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_vecs(docs):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec(doc)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get a baseline as we did before (it should match the one from the previous notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_removal = lambda doc: re.subn(r'@\\w+','', doc.lower())[0]\n",
    "df['Tweet'] = df['Tweet'].map(handle_removal)\n",
    "\n",
    "simple_tokenizer = lambda doc: \" \".join(WordPunctTokenizer().tokenize(doc))\n",
    "df['Tweet'] = df['Tweet'].map(simple_tokenizer)\n",
    "\n",
    "train_data, test_data = train_test_split(df, test_size=0.3, random_state=seed)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "y_train = train_data.Party\n",
    "y_test = test_data.Party\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also get baselines for our previous methods - SVD and PCA. We'll use 300 as the number of components to keep so we can compare with the new technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_data.Tweet)\n",
    "X_test = vectorizer.transform(test_data.Tweet)\n",
    "\n",
    "svd = TruncatedSVD(n_components=300, random_state=seed)\n",
    "svd.fit(X_train)\n",
    "X_train_svd = svd.transform(X_train)\n",
    "X_test_svd =  svd.transform(X_test)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_svd, y_train)\n",
    "y_pred = clf.predict(X_test_svd)\n",
    "print('Truncated SVD Accuracy: {}'.format(accuracy_score(y_pred, y_test)))\n",
    "\n",
    "pca = PCA(n_components=300, random_state=seed)\n",
    "X_train_dense = X_train.toarray()\n",
    "X_test_dense = X_test.toarray()\n",
    "pca.fit(X_train_dense)\n",
    "X_train_pca = pca.transform(X_train_dense)\n",
    "X_test_pca =  pca.transform(X_test_dense)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train_pca, y_train)\n",
    "y_pred = clf.predict(X_test_pca)\n",
    "print('PCA Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 300 features, PCA and SVD have a pretty low accuracy. Now let's split the data and build the vectors - it might take a few minutes to get the vectors for all training and test data. Print the shape of the output vector so we get an idea of the number of features that our model is going to use now. You should see that our feature vector is now of 300 features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = build_sentence_vecs(train_data.Tweet.values)\n",
    "X_test = build_sentence_vecs(test_data.Tweet.values)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the same model and see how much accuracy we can get out of our 300 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy: {}'.format(accuracy_score(y_pred, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 300 features, this is a pretty close accuracy. We can even go further, for example let's try to remove stopwords from the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to download the set of stopwords. Uncomment the below line to run `nltk.download('stopwords')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine functions to use stopwords information\n",
    "\n",
    "def sentvec_tfidf(s, stopwords):\n",
    "    sent = nlp(s)\n",
    "    return np.average(np.array([w.vector for w in sent if w.text not in stopwords]), axis=0)\n",
    "    \n",
    "def build_sentence_vecs_tfidf(docs, stopwords):\n",
    "    num_examples = len(docs)\n",
    "    word_vector_shape = nlp.vocab.vectors.shape[-1]\n",
    "    vectors = np.zeros([num_examples, word_vector_shape])\n",
    "    for ii, doc in enumerate(docs):\n",
    "        vector = sentvec_tfidf(doc, stopwords)\n",
    "        vectors[ii] = vector\n",
    "    \n",
    "    # in case we get any NaN's or Inf, replace them with 0s\n",
    "    return np.nan_to_num(vectors)\n",
    "\n",
    "# Run with english stopwords\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "X_train = build_sentence_vecs_tfidf(train_data.Tweet.values, stopwords)\n",
    "X_test = build_sentence_vecs_tfidf(test_data.Tweet.values, stopwords)\n",
    "\n",
    "clf =  KNeighborsClassifier()\n",
    "clf.fit(X_train, train_data.Party)\n",
    "pred = clf.predict(X_test)\n",
    "print('Accuracy: {}'.format(accuracy_score(pred, test_data.Party)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We got a little bit more accuracy with a simple strategy. There are much more tweaks that you can (and should) try to improve your accuracy. \n",
    "\n",
    "<br> \n",
    "\n",
    "## Final Remarks\n",
    "\n",
    "In this last part, we've shown you that word vectors are pretty useful and intuitive, keeping meaningful information about words in a compact feature space. If you wish to dig further into these word representations we would suggest this [paper](https://arxiv.org/pdf/1301.3781.pdf). As before, take into consideration that although they can be used as an out of the box solution for several NLP tasks, all the factors mentioned before will affect your model performance. So once again, you should still be careful with:\n",
    "\n",
    "- Initial text preprocessing\n",
    "- Choice of classifier\n",
    "- Parameter selection\n",
    "\n",
    "In particular, for most of NLP tasks, neural networks have been showing extremely good performance, and if you really want to get into this field, you should learn more about that. However, these basic techniques are essential to understand some of the reasoning when handling text and can still prove quite useful to us.\n",
    "\n",
    "And that's it for this BLU. You have come out the other side with a much wider view of the different methods and reasoning you can take when handling features in NLP (and outside NLP) in a high dimensional space. There is so much more, but these basic tools should suffice for you to start working with text data and to understand more complex approaches built on top of these methods. See you in the next BLU!\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./media/see-you-in-the-future.png\" width=\"500\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
