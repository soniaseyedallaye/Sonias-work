{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (16, 4)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "from utils import * # We've added all the functions from the last BLU to the utils.py \n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from random import gauss\n",
    "from random import seed\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa import stattools\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Import all Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume_(BTC)</th>\n",
       "      <th>Volume_(Currency)</th>\n",
       "      <th>Weighted_Price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:00:00</th>\n",
       "      <td>973.37</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.35</td>\n",
       "      <td>2.122048</td>\n",
       "      <td>2065.524303</td>\n",
       "      <td>973.363509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:01:00</th>\n",
       "      <td>973.37</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.35</td>\n",
       "      <td>2.122048</td>\n",
       "      <td>2065.524303</td>\n",
       "      <td>973.363509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:02:00</th>\n",
       "      <td>973.37</td>\n",
       "      <td>973.37</td>\n",
       "      <td>973.35</td>\n",
       "      <td>973.35</td>\n",
       "      <td>2.122048</td>\n",
       "      <td>2065.524303</td>\n",
       "      <td>973.363509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:03:00</th>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.36</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>38.934400</td>\n",
       "      <td>973.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-01 00:04:00</th>\n",
       "      <td>973.36</td>\n",
       "      <td>973.40</td>\n",
       "      <td>973.36</td>\n",
       "      <td>973.39</td>\n",
       "      <td>5.458800</td>\n",
       "      <td>5313.529708</td>\n",
       "      <td>973.387871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 23:56:00</th>\n",
       "      <td>8155.00</td>\n",
       "      <td>8155.00</td>\n",
       "      <td>8154.99</td>\n",
       "      <td>8154.99</td>\n",
       "      <td>0.617945</td>\n",
       "      <td>5039.342643</td>\n",
       "      <td>8154.997667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 23:57:00</th>\n",
       "      <td>8154.99</td>\n",
       "      <td>8154.99</td>\n",
       "      <td>8154.00</td>\n",
       "      <td>8154.01</td>\n",
       "      <td>40.655410</td>\n",
       "      <td>331543.193980</td>\n",
       "      <td>8154.958865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 23:58:00</th>\n",
       "      <td>8154.00</td>\n",
       "      <td>8154.01</td>\n",
       "      <td>8150.00</td>\n",
       "      <td>8150.00</td>\n",
       "      <td>9.856911</td>\n",
       "      <td>80340.432933</td>\n",
       "      <td>8150.670628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-26 23:59:00</th>\n",
       "      <td>8150.01</td>\n",
       "      <td>8150.01</td>\n",
       "      <td>8122.82</td>\n",
       "      <td>8145.00</td>\n",
       "      <td>68.274269</td>\n",
       "      <td>555026.852280</td>\n",
       "      <td>8129.370847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-27 00:00:00</th>\n",
       "      <td>8144.99</td>\n",
       "      <td>8145.01</td>\n",
       "      <td>8140.00</td>\n",
       "      <td>8140.00</td>\n",
       "      <td>37.842674</td>\n",
       "      <td>308202.442620</td>\n",
       "      <td>8144.309384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>648001 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Open     High      Low    Close  Volume_(BTC)  \\\n",
       "Timestamp                                                               \n",
       "2017-01-01 00:00:00   973.37   973.37   973.35   973.35      2.122048   \n",
       "2017-01-01 00:01:00   973.37   973.37   973.35   973.35      2.122048   \n",
       "2017-01-01 00:02:00   973.37   973.37   973.35   973.35      2.122048   \n",
       "2017-01-01 00:03:00   973.36   973.36   973.36   973.36      0.040000   \n",
       "2017-01-01 00:04:00   973.36   973.40   973.36   973.39      5.458800   \n",
       "...                      ...      ...      ...      ...           ...   \n",
       "2018-03-26 23:56:00  8155.00  8155.00  8154.99  8154.99      0.617945   \n",
       "2018-03-26 23:57:00  8154.99  8154.99  8154.00  8154.01     40.655410   \n",
       "2018-03-26 23:58:00  8154.00  8154.01  8150.00  8150.00      9.856911   \n",
       "2018-03-26 23:59:00  8150.01  8150.01  8122.82  8145.00     68.274269   \n",
       "2018-03-27 00:00:00  8144.99  8145.01  8140.00  8140.00     37.842674   \n",
       "\n",
       "                     Volume_(Currency)  Weighted_Price  \n",
       "Timestamp                                               \n",
       "2017-01-01 00:00:00        2065.524303      973.363509  \n",
       "2017-01-01 00:01:00        2065.524303      973.363509  \n",
       "2017-01-01 00:02:00        2065.524303      973.363509  \n",
       "2017-01-01 00:03:00          38.934400      973.360000  \n",
       "2017-01-01 00:04:00        5313.529708      973.387871  \n",
       "...                                ...             ...  \n",
       "2018-03-26 23:56:00        5039.342643     8154.997667  \n",
       "2018-03-26 23:57:00      331543.193980     8154.958865  \n",
       "2018-03-26 23:58:00       80340.432933     8150.670628  \n",
       "2018-03-26 23:59:00      555026.852280     8129.370847  \n",
       "2018-03-27 00:00:00      308202.442620     8144.309384  \n",
       "\n",
       "[648001 rows x 7 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###train and test must be in the form of sereies so if we have a data frame we need to choose a column of it\n",
    "# concatenate train and test\n",
    "#df_ = pd.concat([train, test], axis=0)\n",
    "#read data\n",
    "dataset = 'dataset name'\n",
    "#df_ = pd.read_csv(os.path.join('data',dataset + \".csv\"))  \n",
    "df_ = pd.read_csv('data/bitcoin.csv')\n",
    "df = df_.copy()\n",
    "\n",
    "\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%Y-%m-%d', infer_datetime_format=True)\n",
    "df = df.set_index('Timestamp')\n",
    "df = df.sort_index()\n",
    "df\n",
    "# then seperate train and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing(Feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the missing dates with zero and choosing one column to have series\n",
    "df = df.resample('MS').asfreq()#asfreq() put all the new rows nan value\n",
    "df_resampled = df.resample('D').mean()   #can be 'M','D','Y','W' or even df.resample('5 min').sum().plot()\n",
    "data.isnull().sum()\n",
    "df_cleaned = df_resampled.fillna(0)\n",
    "df_cleaned = df_cleaned.customers\n",
    "###filling the missing value with the method of forward fill\n",
    "df['col with nans'].fillna(method='ffill').plot() #we can plot it also\n",
    "#filling the missing value with KNN imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer() # To obtain better results we may want to have more neighbors, of course that in a real setting\n",
    "                       # this is a parameter that should be tunned\n",
    "    \n",
    "data_imputed = imputer.fit_transform(df) # This may take a bit to run\n",
    "df_imputed = pd.DataFrame(data_imputed, index=df.index, columns=df.columns)\n",
    "df_imputed.'col with nans'.interpolate().plot() # we can plot it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding day weekday etc to data frame\n",
    "#df['day'] = df['date'].dt.day\n",
    "#df['month'] = df['date'].dt.month\n",
    "#df['year'] = df['date'].dt.year\n",
    "#df['hour'] = df['date'].dt.hour\n",
    "#df['minute'] = df['date'].dt.minute\n",
    "#df['second'] = df['date'].dt.second\n",
    "#df['day of the week'] = df['date'].dt.weekday\n",
    "#df['day of the week name'] = df['date'].dt.day_name()\n",
    "#df['quarter'] = df['date'].dt.quarter\n",
    "#df['is it a leap year?'] = df['date'].dt.is_leap_year\n",
    "\n",
    "#compute comulative sum\n",
    "\n",
    "#df['a column name'].cumsum().plot();  # the total volume traded since the start \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some question\n",
    "#What was the weekly change in price, over time?#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering (adding more features to  data frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#diff\n",
    "df_cleaned['diff'] = df_cleaned.diff()\n",
    "#Rolling windows\n",
    "df_cleaned['rolling_max'] = df_cleaned['customers(some columns)'].rolling('7D').max()\n",
    "df_cleaned['rolling_min'] = df_cleaned['customers(some columns)'].rolling('7D').min()\n",
    "df_cleaned['rolling_mean'] = df_cleaned['customers(some columns)'].rolling('7D').mean()\n",
    "df_cleaned['rolling_std'] = df_cleaned['customers(some columns)'].rolling('7D').std()\n",
    "#add Holiday if it is 1 and if it is not 0\n",
    "holidays = df_cleaned[((df_cleaned.index.month==12) & (df_cleaned.index.day==25))\n",
    "              |((df_cleaned.index.month==1) & (df_cleaned.index.day==1))].customers\n",
    "df_cleaned['holidays'] = holidays + 1\n",
    "df_cleaned['holidays'] = df_cleaned['holidays'].fillna(0)\n",
    "#day of week and month\n",
    "df_cleaned['day_of_week'] = df_cleaned.index.weekday\n",
    "df_cleaned['month'] = df_cleaned.index.month\n",
    "#Circular encoding\n",
    "\n",
    "df_cleaned['sin_weekday'] = np.sin(2*np.pi*df_cleaned.index.weekday/7)\n",
    "df_cleaned['cos_weekday'] = np.cos(2*np.pi*df_cleaned.index.weekday/7)\n",
    "        \n",
    "df_cleaned['sin_month'] = np.sin(2*np.pi*df_cleaned.index.month/12)\n",
    "df_cleaned['cos_month'] = np.cos(2*np.pi*df_cleaned.index.month/12)\n",
    "df_cleaned = df_cleaned.drop(['day_of_week','month'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If we have multi index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.set_index(['col1','col2'])\n",
    "data.sort_index()\n",
    "#group by multi index\n",
    "food.groupby(level='Product').Price.idxmin()\n",
    "#selecting from multi index\n",
    "#Selecting from multi-index: dataframe.loc[idx[index_level_0, index_level_1], columns]\n",
    "idx = pd.IndexSlice\n",
    "data.loc[idx[:,'msft'],:]\n",
    "#how to get indexes\n",
    "data.loc[data.index.get_level_values(0).day_name()=='Monday'].Close.mean()\n",
    "#percentage change\n",
    "data.sort_index(level=['Stock', 'Date']).Close.pct_change().head()\n",
    "# group by the stock, and then take the Close, and calculate percentage change \n",
    "data.groupby(level='Stock').Close.pct_change().head()\n",
    "#Put one of the multi-index level into columns: unstack()\n",
    "# group by using lambda function\n",
    "food.groupby(level='Product').Price.agg(lambda x: list(x.loc[x == x.min()].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot of one column with respect to another one\n",
    "plt.scatter(data['thousands of passengers'], data.lag_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between one coulumns with its lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It needs resample .asfriq in advance to fill the missing date with nans and then those nans will be ignored by corr function\n",
    "data.corr()['thousands of passengers(it is a column of df)']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal decomposition in time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "#To choose modell aditive or multiplicetive we have to plot the follwing:\n",
    "df.rolling(24).std().plot();#24 is seasonal.It can be different\n",
    "#If variance increasing model = Multiplicative if variance is constant model = additive\n",
    "decomposition = seasonal_decompose(df, model='multiplicative')\n",
    "decomposition.plot()\n",
    "#To see the head of seasonal , trend , ... data\n",
    "decomposition.trend.head(10)\n",
    "decomposition.seasonal.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "acf(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 95% confidence interval.(this is beacause of alpha = 0.5)\n",
    "plot_acf(df, alpha=.05,lags=50)\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pacf(df, alpha=0.05, lags=50, method='ols')\n",
    "plt.xlabel('lag')\n",
    "plt.ylabel('Autocorrelation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A stationary time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean, variance, autocorrelation, etc. are all constant over time.\n",
    "#1-stabilize variance\n",
    "df_logg = df.map(np.log)\n",
    "#2-Removing Trend\n",
    "df_logg_diff = df_logg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating stationarity(Dickey-Fuller test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Dickey-Fuller test:Law pvalue mean the time series is stationary The p value must be less than alpha\n",
    "from statsmodels.tsa.stattools import adfuller.diff()\n",
    "adfstat, pvalue, usedlag, nobs, critvalues, icbest = adfuller(df_logg_diff.dropna())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
