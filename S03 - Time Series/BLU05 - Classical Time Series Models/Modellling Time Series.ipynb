{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot as plt \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "%matplotlib inline \n",
    "import numpy as np\n",
    "plt.rcParams['figure.figsize'] = (16, 4)\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "warnings.filterwarnings(action=\"ignore\", module=\"scipy\", message=\"^internal gelsd\")\n",
    "\n",
    "from utils import * # We've added all the functions from the last BLU to the utils.py \n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from random import gauss\n",
    "from random import seed\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import itertools\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa import stattools\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 4)\n",
    "import pmdarima as pm\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "idx = pd.IndexSlice\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)   \n",
    "from random import seed\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import itertools\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa import stattools\n",
    "import hashlib # for grading purposes\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Befor modelling we do need to make time series stationary(we do log part but diff part will be done in the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax = pm.AutoARIMA(trace=True, supress_warnings=True, m=12)#m=12 because of seasonality we need to change according to that\n",
    "sarimax.fit(train)\n",
    "sarimax_forecast = sarimax.predict(24)#24 is len(test). This model predict 24 times after training set which is test set\n",
    "np.exp(sarimax_forecast)\n",
    "#In sample prediction \n",
    "predictions =np.exp(sarimax.predict_in_sample())\n",
    "mae =mean_absolute_error(predictions,np.exp(train_log))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple one-step forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_forecast_one_step_refit = []\n",
    "sarimax.fit(train)        # Note that we're fitting the autoArima again. This is to make sure you always \n",
    "for i in np.arange(0,24): # run it before the rest of the loop\n",
    "    next_step_forecast = sarimax.predict(1)[0]\n",
    "    sarimax_forecast_one_step_refit.append(next_step_forecast)\n",
    "    sarimax = sarimax.update(test[i:i+1])\n",
    "mean_absolute_error(np.exp(sarimax_forecast_one_step_refit),np.exp(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi step forcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orecast = np.exp(sarimax.predict(3*12,emissions_test[:]))#3*12 is the size of test considering seasonality\n",
    "mae = mean_absolute_error(forecast,emissions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# multiple one-step forecasts with exog input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarimax_forecast_one_step_refit = []\n",
    "sarimax.fit(emissions_train_log,exog_train) \n",
    "for i in np.arange(0,len(exog_test)): \n",
    "    next_step_forecast = sarimax.predict(1,exog_test[i:i+1])[0]\n",
    "    sarimax_forecast_one_step_refit.append(next_step_forecast)\n",
    "    sarimax = sarimax.update(emissions_test_logged[i:i+1],exog_test[i:i+1])\n",
    "#MAE\n",
    "sarimax_forecast_one_step_refit_exp = np.exp(sarimax_forecast_one_step_refit)\n",
    "mae = mean_absolute_error(sarimax_forecast_one_step_refit_exp,exog_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boxcox Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best transformation to unskew our distribution.\n",
    "from pmdarima.pipeline import Pipeline\n",
    "from pmdarima.preprocessing import BoxCoxEndogTransformer\n",
    "\n",
    "sarimax_pipeline = Pipeline([\n",
    "    ('boxcox', BoxCoxEndogTransformer(lmbda2=1)),\n",
    "    ('arima', pm.AutoARIMA(trace=True,\n",
    "                           suppress_warnings=True,\n",
    "                           m=24, \n",
    "                           method='nm', # More data means longer let's use 'nm' to make it faster\n",
    "                           maxiter=20, # Let's also reduce maxiter to make it faster.\n",
    "                           \n",
    "                          )\n",
    "    )\n",
    "])\n",
    "\n",
    "sarimax_pipeline.fit(train)\n",
    "mean_absolute_error(sarimax_pipeline.predict(24),test)\n",
    "\n",
    "#find the best transformation to unskew our distribution but with exogenous data\n",
    "#(we need to do same preprocessing for exogenous data)\n",
    "\n",
    "sarimax_pipeline_exog = Pipeline([  #The boxcox transform has been removed because it messes with the exogenous input\n",
    "    ('arima', pm.AutoARIMA(trace=True,\n",
    "                           suppress_warnings=True,\n",
    "                           m=24, \n",
    "                           method='nm',\n",
    "                           maxiter=20,\n",
    "                           \n",
    "                          )\n",
    "    )\n",
    "])\n",
    "\n",
    "sarimax_pipeline_exog.fit(train,exog_train)\n",
    "#predict in sample\n",
    "sarimax_pipeline_exog.predict_in_sample(exog_train)\n",
    "#predict test\n",
    "sarimax_pipeline_exog.predict(29*24,exog_test)\n",
    "#predict test with confident interval\n",
    "preds = sarimax_pipeline_exog.predict(24*7,exog_test[:24*7], return_conf_int=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some functions usefull for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_target(series_, number_of_periods_ahead):\n",
    "    \"\"\" \n",
    "    takes a series, turned it into a dataframe, and adds a new column called target\n",
    "    This column is the input series, lagged number_of_periods_ahead into the future\n",
    "    \"\"\"\n",
    "    \n",
    "    # make a copy \n",
    "    series_ = series_.copy()\n",
    "    series_.name = 'customers'\n",
    "    \n",
    "    # make a dataframe from the series\n",
    "    df_ = pd.DataFrame(series_)\n",
    "    \n",
    "    # the target column will be the input series, lagged into the future\n",
    "    df_['target'] = series_.shift(-number_of_periods_ahead)\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_last_day(df_):\n",
    "    \n",
    "    \"\"\"\n",
    "    takes a dataset which has the target and features built \n",
    "    and separates it into the last day\n",
    "    \"\"\"\n",
    "    # take the last period \n",
    "    last_period = df_.iloc[-1]\n",
    "    \n",
    "    # the last period is now a series, so it's name will be the timestamp\n",
    "    training_data = df_.loc[df_.index < last_period.name]\n",
    "\n",
    "    return last_period, training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_some_features(df_, num_periods_lagged=1, num_periods_diffed=0, weekday=False, month=False, rolling=[], holidays=False): \n",
    "    \"\"\"\n",
    "    Builds some features by calculating differences between periods  \n",
    "    \"\"\"\n",
    "    # make a copy \n",
    "    df_ = df_.copy()\n",
    "        \n",
    "    # for a few values, get the lags  \n",
    "    for i in range(1, num_periods_lagged+1):\n",
    "        # make a new feature, with the lags in the observed values column\n",
    "        df_['lagged_%s' % str(i)] = df_['customers'].shift(i)\n",
    "        \n",
    "    # for a few values, get the diffs  \n",
    "    for i in range(1, num_periods_diffed+1):\n",
    "        # make a new feature, with the lags in the observed values column\n",
    "        df_['diff_%s' % str(i)] = df_['customers'].diff(i)\n",
    "    \n",
    "    for stat in rolling:\n",
    "        df_['rolling_%s'%str(stat)] = df_['customers'].rolling('7D').aggregate(stat)\n",
    "        \n",
    "    if weekday == True:\n",
    "        df_['sin_weekday'] = np.sin(2*np.pi*df_.index.weekday/7)\n",
    "        df_['cos_weekday'] = np.sin(2*np.pi*df_.index.weekday/7)\n",
    "        \n",
    "    if month == True:\n",
    "        df_['sin_month'] = np.sin(2*np.pi*df_.index.month/12)\n",
    "        df_['cos_month'] = np.sin(2*np.pi*df_.index.month/12)\n",
    "        \n",
    "    if holidays == True:\n",
    "        holidays = df_[((df_.index.month==12) & (df_.index.day==25))\n",
    "              |((df_.index.month==1) & (df_.index.day==1))].customers\n",
    "        df_['holidays'] = holidays + 1\n",
    "        df_['holidays'] = df_['holidays'].fillna(0)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_train_and_test_set(last_period_, training_data_, target='target'): \n",
    "    \n",
    "    \"\"\" \n",
    "    separates training and test set (clue was in the name, really... )\n",
    "    Ok, we were lazy and left the target hardcoded as 'target'. Shame on us. \n",
    "    \"\"\"\n",
    "    \n",
    "    # anything that isn't a target is a feature \n",
    "    features = [feature for feature in training_data_.columns if feature != target]\n",
    "    \n",
    "    # adding a sneaky little dropna to avoid the missing data problem above \n",
    "    X_train = training_data_.dropna()[features]\n",
    "    y_train = training_data_.dropna()[target]\n",
    "    \n",
    "    X_last_period = last_period_[features]\n",
    "    \n",
    "    return X_train, y_train, X_last_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_prediction(series_, number_of_periods_ahead, num_periods_lagged, num_periods_diffed, weekday, month, rolling, holidays):\n",
    "    \n",
    "    \"\"\" \n",
    "    Wrapper to go from the original series to X_train, y_train, X_last_period \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # build the target \n",
    "    data_with_target = build_target(series_, \n",
    "                                    number_of_periods_ahead)\n",
    "    \n",
    "    # build the features \n",
    "    data_with_target_and_features = build_some_features(data_with_target, \n",
    "                                                        num_periods_lagged=num_periods_lagged,\n",
    "                                                       num_periods_diffed=num_periods_diffed,\n",
    "                                                       weekday=weekday,\n",
    "                                                       month=month,\n",
    "                                                       rolling=rolling,\n",
    "                                                       holidays=holidays)\n",
    "    # separate train and test data \n",
    "    last_period, training_data = separate_last_day(data_with_target_and_features)\n",
    "\n",
    "    # separate X_train, y_train, and X_test \n",
    "    X_train, y_train, X_last_period = separate_train_and_test_set(last_period, \n",
    "                                                           training_data, \n",
    "                                                           target='target')\n",
    "    \n",
    "    # return ALL OF THE THINGS! (well, actually just the ones we need)\n",
    "    return X_train, y_train, X_last_period "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_period_n(series_, model, number_of_periods_ahead, num_periods_lagged, num_periods_diffed, weekday, month, rolling, holidays): \n",
    "    \n",
    "        X_train, y_train, X_last_period = prepare_for_prediction(series_, \n",
    "                                                             number_of_periods_ahead, \n",
    "                                                             num_periods_lagged,\n",
    "                                                             num_periods_diffed,\n",
    "                                                             weekday,\n",
    "                                                             month,\n",
    "                                                             rolling,\n",
    "                                                             holidays)\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        return model.predict(X_last_period.values.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_n_periods(series_, n_periods, model, num_periods_lagged, num_periods_diffed=0, weekday=False, month=False,rolling=[], holidays=False): \n",
    "    predictions = []\n",
    "\n",
    "    for period_ahead in range(1, n_periods+1):\n",
    "        pred = predict_period_n(series_=series_, \n",
    "                                model=model, \n",
    "                                number_of_periods_ahead=period_ahead, \n",
    "                                num_periods_lagged=num_periods_lagged,\n",
    "                                num_periods_diffed=num_periods_diffed,\n",
    "                                weekday=weekday,\n",
    "                                month=month,\n",
    "                                rolling=rolling,\n",
    "                                holidays=holidays)\n",
    "        \n",
    "        predictions.append(pred[0])\n",
    "        \n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation, which model parameters are better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {'model': [LinearRegression(), GradientBoostingRegressor()], \n",
    "              'num_periods_lagged':np.arange(1,3),\n",
    "              'num_periods_diffed':np.arange(0,3),\n",
    "              'weekday':[True,False],\n",
    "              'month':[True,False],\n",
    "              'holidays': [True],\n",
    "              'rolling' : [[np.mean,np.min,np.max,np.std]]\n",
    "             }\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "###Seperate train test and validation\n",
    "error_lst = []\n",
    "\n",
    "for params in tqdm(grid):\n",
    "    predictions = predict_n_periods(series_=train, \n",
    "                    n_periods=92, \n",
    "                      model=params['model'], \n",
    "                      num_periods_lagged=params['num_periods_lagged'],\n",
    "                      num_periods_diffed=params['num_periods_diffed'],\n",
    "                      weekday=params['weekday'],\n",
    "                      month=params['month'],\n",
    "                      rolling=[np.mean,np.max,np.min]\n",
    "                    )\n",
    "\n",
    "    error_lst.append(mean_absolute_error(val,predictions))\n",
    "pd.Series(error_lst).idxmin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding train and validation\n",
    "train = train.append(val)\n",
    "\n",
    "predictions = predict_n_periods(series_=train, \n",
    "                  n_periods=len(test), \n",
    "                  model=GradientBoostingRegressor(), \n",
    "                  num_periods_lagged=2,\n",
    "                  num_periods_diffed=0,\n",
    "                  weekday=True,\n",
    "                  month=False,\n",
    "                  rolling=[np.mean,np.min,np.max,np.std],\n",
    "                  holidays=True\n",
    "                  )\n",
    "mean_absolute_error(test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is not good at all. We will have data lekage with this kind of cross validation we need to use other methods \n",
    "#Import TimeSeriesSplit\n",
    "#from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "#cv = KFold(n_splits=10)\n",
    "\n",
    "# Iterate through CV splits\n",
    "#fig, ax = plt.subplots()\n",
    "#for ii, (tr, tt) in enumerate(cv.split(store_train/store_train.max())):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    #ax.plot(store_train.index[tr], ii + store_train[tr]/store_train.max(), color='green')\n",
    "    #ax.plot(store_train.index[tt], ii + store_train[tt]/store_train.max(), color='orange')\n",
    "\n",
    "#ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding Window cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the fold for validation are similar to k-fold cross validation but it doese not consider the training set after the validation\n",
    "#set so we do not have data lekage in this case\n",
    "#This is better than k-fold...\n",
    "\n",
    "# Import TimeSeriesSplit\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Create time-series cross-validation object\n",
    "cv = TimeSeriesSplit(n_splits=20)\n",
    "\n",
    "# Iterate through CV splits\n",
    "fig, ax = plt.subplots()\n",
    "for ii, (tr, tt) in enumerate(cv.split(store_train/store_train.max())):\n",
    "    # Plot the training data on each iteration, to see the behavior of the CV\n",
    "    ax.plot(store_train.index[tr], ii + store_train[tr]/store_train.max(), color='green')\n",
    "    ax.plot(store_train.index[tt], ii + store_train[tt]/store_train.max(), color='orange')\n",
    "\n",
    "ax.set(title='Training data on each CV iteration', ylabel='CV iteration')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the modell using expandig windows cross validation and see with different train and validation this cross validation consider,\n",
    "#what happend to the mean absolute error\n",
    "X = store_train\n",
    "\n",
    "# Iterate through CV splits\n",
    "n_splits = 20\n",
    "cv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "idx_lst = []\n",
    "values_lst = []\n",
    "\n",
    "for ii, (tr, tt) in tqdm(enumerate(cv.split(X))):\n",
    "    # Fit the model on training data and collect the coefficients\n",
    "    train = X[tr]\n",
    "    test = X[tt]\n",
    "    idx_lst.append(X.index[tt][0])\n",
    "    error_lst = []\n",
    "    predictions = predict_n_periods(series_=train, \n",
    "                    n_periods=len(test), \n",
    "                    model=GradientBoostingRegressor(n_estimators=20, learning_rate=0.5), \n",
    "                    num_periods_lagged=2,\n",
    "                    num_periods_diffed=0,\n",
    "                    weekday=True,\n",
    "                    month=False)\n",
    "    values_lst.append(mean_absolute_error(test,predictions))\n",
    "    pd.Series(values_lst, index=idx_lst).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: explorer.exe.: command not found\r\n"
     ]
    }
   ],
   "source": [
    "! explorer.exe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!explorer.exe ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
